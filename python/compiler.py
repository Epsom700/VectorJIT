from .ops import Node, Operators
from typing import List, Optional, Set, Dict
import subprocess
import ctypes
import numpy as np
import os
import tempfile
class Compiler: 
    _binary_cache: Dict[str, ctypes.CDLL] = {}
    def __init__(self): 
        self.visited_nodes: Set[int] = set()
        self.execution_order: List[Node] = []
        self.code_buffer: List[str] = []
        self.var_map: Dict[int, str] = {}
        self.temp_file: List[str] = []
        
    def _get_cache_key(self, node: Node, is_backward: bool = False) -> str:
        """Generates a unique key based on the node structure and shapes."""
        input_shapes = tuple(n.shape for n in node.inputs)
        # Include attributes (like eps in LayerNorm) in the hash
        attr_str = str(sorted(node.attributes.items()))
        return f"{node.op_type.name}_{node.shape}_{input_shapes}_{attr_str}_back={is_backward}"

    def _topological_sort(self, node: Node) -> List[Node]:
        if node.id in self.visited_nodes: 
            return 
        
        self.visited_nodes.add(node.id)

        is_fused = node.op_type in (Operators.MATMUL, Operators.LAYERNORM, Operators.SOFTMAX_CROSS_ENTROPY)
        
        if not is_fused:
            for input_node in node.inputs: 
                self._topological_sort(input_node)

        self.execution_order.append(node)
        return self.execution_order  # Return for convenience

    def compile(self, root_node: Node) -> str: 
        self.visited_nodes.clear()
        self.execution_order.clear()
        self.code_buffer.clear()
        self._topological_sort(root_node)
        
        # Create map for O(1) index lookup
        self.node_idx_map = {node.id: i for i, node in enumerate(self.execution_order)}

        if root_node.op_type == Operators.MATMUL: 
            return self.compile_matMul_SIMD(root_node)
        
        if root_node.op_type == Operators.LAYERNORM: 
            return self.compile_LayerNorm_SIMD(root_node)
        
        if root_node.op_type == Operators.SOFTMAX_CROSS_ENTROPY:
            return self.compile_fused_loss_forward(root_node)
        
        # 1. Header & Shared State
        self.code_buffer = [
            "#include <iostream>",
            "#include <vector>",
            "#include <cmath>",
            "#include <arm_neon.h>",
            "#include <omp.h>", 
            "",
            "",
            "// Generated by VectorJIT",
            "extern \"C\" void compute(float** vars, int n) {",
            "    int vector_limit = n - (n % 4);", 
            "    float32x4_t zero_vec = vdupq_n_f32(0.0f);",
            "    #pragma omp parallel for"
            ""
        ]

        # 2. VECTOR LOOP
        # Use <= to ensure we process exact multiples of 4
        self.var_map.clear()
        self.code_buffer.append("    for (int i = 0; i < vector_limit; i += 4) {")
        
        for node in self.execution_order: 
            self._emit_vector_node_code(node)

        self.code_buffer.append("    }")

        # 3. SCALAR TAIL LOOP
        self.var_map.clear()
        self.code_buffer.append("    // Scalar Cleanup")
        self.code_buffer.append("    for (int i = vector_limit; i < n; ++i) {")
        
        # We must re-emit scalar code because vector variables (float32x4_t) 
        # are not compatible with scalar math.
        for node in self.execution_order: 
            self._emit_node_code(node)
        
        self.code_buffer.append("    }")
        self.code_buffer.append("}")
        
        return "\n".join(self.code_buffer)
    
    def compile_matmul(self, node: Node) -> str:
        input_a = node.inputs[0]
        input_b = node.inputs[1]
        
        M = input_a.shape[0]
        K = input_a.shape[1]
        N = input_b.shape[1]
        
        # Resolve Input Indices
        idx_a = 0 if input_a.name == "A" else 1
        idx_b = 1 if idx_a == 0 else 0
        
        # BLOCK_SIZE: 32 floats * 32 floats * 4 bytes = 4KB.
        # This fits easily into L1 Cache (typically 32KB - 64KB).
        BLOCK_SIZE = 32

        code = [
            "#include <iostream>",
            "#include <omp.h>",
            "#include <algorithm>", # Required for std::min, std::fill_n
            "",
            "extern \"C\" void compute(float* output, float** inputs, int n) {",
            f"    const int M = {M};",
            f"    const int K = {K};",
            f"    const int N = {N};",
            f"    const int BLOCK_SIZE = {BLOCK_SIZE};",
            "    ",
            "    float* A = inputs[" + str(idx_a) + "];",
            "    float* B = inputs[" + str(idx_b) + "];",
            "    ",
            "    // 1. Initialize Output to Zero (Critical for accumulation)",
            "    // We use std::fill_n which is often optimized to memset",
            "    std::fill_n(output, M * N, 0.0f);",
            "    ",
            "    // 2. Tiled Matrix Multiplication",
            "    // Collapse the outer two loops to parallelize blocks across threads",
            "    #pragma omp parallel for collapse(2)",
            "    for (int ii = 0; ii < M; ii += BLOCK_SIZE) {",
            "        for (int jj = 0; jj < N; jj += BLOCK_SIZE) {",
            "            for (int kk = 0; kk < K; kk += BLOCK_SIZE) {",
            "                ",
            "                // 3. Inner Loops (Process one Block)",
            "                // Use std::min to handle boundary conditions",
            "                for (int i = ii; i < std::min(ii + BLOCK_SIZE, M); ++i) {",
            "                    for (int j = jj; j < std::min(jj + BLOCK_SIZE, N); ++j) {",
            "                        ",
            "                        // Load current value (if we were accumulating into output directly)",
            "                        // Use a temp 'current_val' for the inner K loop",
            "                        float current_val = output[i * N + j];",
            "                        ",
            "                        for (int k = kk; k < std::min(kk + BLOCK_SIZE, K); ++k) {",
            "                            current_val += A[i * K + k] * B[k * N + j];",
            "                        }",
            "                        output[i * N + j] = current_val;",
            "                    }",
            "                }",
            "            }",
            "        }",
            "    }",
            "}"
        ]
        return "\n".join(code)
    
    def compile_matMul_SIMD(self, node: Node) -> str: 
        input_a = node.inputs[0]
        input_b = node.inputs[1]
        M = input_a.shape[0]
        K = input_a.shape[1]
        N = input_b.shape[1]
        
        # Resolve Input Indices
        idx_a = 0 if input_a.name == "A" else 1
        idx_b = 1 if idx_a == 0 else 0
        
        # BLOCK_SIZE: 32 floats * 32 floats * 4 bytes = 4KB.
        # This fits easily into L1 Cache (typically 32KB - 64KB).
        BLOCK_SIZE = 32

        code = [
            "#include <iostream>",
            "#include <arm_neon.h>",
            "#include <omp.h>",
            "#include <algorithm>", # Required for std::min, std::fill_n
            "",
            "extern \"C\" void compute(float** vars, int n) {",
            f"    const int M = {M};",
            f"    const int N = {N}; ", 
            f"    const int K = {K};", 
            f"    const int BLOCK_SIZE = {BLOCK_SIZE};", 
            "   ", 
            "   ", 
            "   float* output = vars[0];",
            "   float* A = vars[1];", 
            "   float* B = vars[2]; ", 
            "   std::fill_n(output, M*N, 0.0f);", 
            "   #pragma omp parallel for collapse(2)", 
            "   for(int I = 0; I < M; I+=BLOCK_SIZE){",
            "       for (int J = 0; J < N; J+=BLOCK_SIZE){", 
            "           for (int L = 0; L < K;  L+=BLOCK_SIZE){", 
            "               int i_limit = std::min(I + BLOCK_SIZE, M);", 
            "               int j_limit = std::min(J + BLOCK_SIZE, N);", 
            "               int k_limit = std::min(L + BLOCK_SIZE, K); ", 
            "               for (int i = I; i< i_limit; ++i){", 
            "                   for(int k=L; k<k_limit; ++k){", 
            "                       float32x4_t vec_a = vdupq_n_f32(A[i * K + k]);", 
            "                       int j = J;", 
            "                       for (; j <= j_limit -4 ; j+=4){", 
            "                           float32x4_t vec_c = vld1q_f32(&output[i * N + j ]);", 
            "                           float32x4_t vec_b = vld1q_f32(&B[k*N+j]);", 
            "                           vec_c = vfmaq_f32(vec_c, vec_a, vec_b);", 
            "                           vst1q_f32(&output[i*N+j], vec_c);", 
            "                       }",
            "                       for (; j < j_limit; ++j){", 
            "                           output[i * N + j]+=A[i * K + k] * B[k * N + j];", 
            "                       }", 
            "                   }", 
            "               }", 
            "           }", 
            "       }", 
            "   }", 
            "}"

        ]

        return "\n".join(code)

    def _emit_vector_node_code(self, node: Node) -> None: 
        var_name = f"vec_{node.id}"
        idx = self.node_idx_map[node.id]

        if node.op_type == Operators.INPUT: 
            self.var_map[node.id] = var_name # Only register upon emission
            self.code_buffer.append(f"        float32x4_t {var_name} = vld1q_f32(&vars[{idx}][i]);")
            
        elif node.op_type == Operators.RELU:
            input_node = node.inputs[0]
            input_var = self.var_map.get(input_node.id)
            if not input_var:
                in_idx = self.node_idx_map[input_node.id]
                input_var = f"vec_pre_{input_node.id}"
                self.code_buffer.append(f"        float32x4_t {input_var} = vld1q_f32(&vars[{in_idx}][i]);")
                
            self.var_map[node.id] = var_name
            self.code_buffer.append(f"        float32x4_t {var_name} = vmaxq_f32({input_var}, zero_vec);")
            self.code_buffer.append(f"        vst1q_f32(&vars[{idx}][i], {var_name});")

        elif node.op_type == Operators.ADD:
            left_node, right_node = node.inputs
            left_var = self.var_map.get(left_node.id)
            right_var = self.var_map.get(right_node.id)
            if not left_var:
                in_idx = self.node_idx_map[left_node.id]
                left_var = f"vec_pre_{left_node.id}"
                self.code_buffer.append(f"        float32x4_t {left_var} = vld1q_f32(&vars[{in_idx}][i]);")
            if not right_var:
                in_idx = self.node_idx_map[right_node.id]
                right_var = f"vec_pre_{right_node.id}"
                self.code_buffer.append(f"        float32x4_t {right_var} = vld1q_f32(&vars[{in_idx}][i]);")

            self.var_map[node.id] = var_name
            self.code_buffer.append(f"        float32x4_t {var_name} = vaddq_f32({left_var}, {right_var});")
            self.code_buffer.append(f"        vst1q_f32(&vars[{idx}][i], {var_name});")

        elif node.op_type == Operators.SUB:
            left_node, right_node = node.inputs
            left_var = self.var_map.get(left_node.id)
            right_var = self.var_map.get(right_node.id)
            if not left_var:
                in_idx = self.node_idx_map[left_node.id]
                left_var = f"vec_pre_{left_node.id}"
                self.code_buffer.append(f"        float32x4_t {left_var} = vld1q_f32(&vars[{in_idx}][i]);")
            if not right_var:
                in_idx = self.node_idx_map[right_node.id]
                right_var = f"vec_pre_{right_node.id}"
                self.code_buffer.append(f"        float32x4_t {right_var} = vld1q_f32(&vars[{in_idx}][i]);")

            self.var_map[node.id] = var_name
            self.code_buffer.append(f"        float32x4_t {var_name} = vsubq_f32({left_var}, {right_var});")
            self.code_buffer.append(f"        vst1q_f32(&vars[{idx}][i], {var_name});")
        elif node.op_type == Operators.MUL:
            left_node, right_node = node.inputs
            left_var = self.var_map.get(left_node.id)
            right_var = self.var_map.get(right_node.id)
            if not left_var:
                in_idx = self.node_idx_map[left_node.id]
                left_var = f"vec_pre_{left_node.id}"
                self.code_buffer.append(f"        float32x4_t {left_var} = vld1q_f32(&vars[{in_idx}][i]);")
            if not right_var:
                in_idx = self.node_idx_map[right_node.id]
                right_var = f"vec_pre_{right_node.id}"
                self.code_buffer.append(f"        float32x4_t {right_var} = vld1q_f32(&vars[{in_idx}][i]);")

            self.var_map[node.id] = var_name
            self.code_buffer.append(f"        float32x4_t {var_name} = vmulq_f32({left_var}, {right_var});")
            self.code_buffer.append(f"        vst1q_f32(&vars[{idx}][i], {var_name});")

        elif node.op_type == Operators.DIV:
            left_node, right_node = node.inputs
            left_var = self.var_map.get(left_node.id)
            right_var = self.var_map.get(right_node.id)
            if not left_var:
                in_idx = self.node_idx_map[left_node.id]
                left_var = f"vec_pre_{left_node.id}"
                self.code_buffer.append(f"        float32x4_t {left_var} = vld1q_f32(&vars[{in_idx}][i]);")
            if not right_var:
                in_idx = self.node_idx_map[right_node.id]
                right_var = f"vec_pre_{right_node.id}"
                self.code_buffer.append(f"        float32x4_t {right_var} = vld1q_f32(&vars[{in_idx}][i]);")

            self.var_map[node.id] = var_name
            self.code_buffer.append(f"        float32x4_t {var_name} = vdivq_f32({left_var}, {right_var});")
            self.code_buffer.append(f"        vst1q_f32(&vars[{idx}][i], {var_name});")

    def _emit_node_code(self, node: Node) -> None: 
        var_name = f"var_{node.id}"
        idx = self.node_idx_map[node.id]

        if node.op_type == Operators.INPUT:
            self.var_map[node.id] = var_name
            self.code_buffer.append(f"        float {var_name} = vars[{idx}][i];")
            
        elif node.op_type in (Operators.ADD, Operators.SUB, Operators.MUL, Operators.DIV, Operators.RELU):
            # Resolve inputs from map or memory
            input_vars = []
            for inp in node.inputs:
                if inp.id in self.var_map:
                    input_vars.append(self.var_map[inp.id])
                else:
                    in_idx = self.node_idx_map[inp.id]
                    tmp_v = f"var_pre_{inp.id}"
                    self.code_buffer.append(f"        float {tmp_v} = vars[{in_idx}][i];")
                    input_vars.append(tmp_v)

            self.var_map[node.id] = var_name
            if node.op_type == Operators.RELU:
                self.code_buffer.append(f"        float {var_name} = ({input_vars[0]} > 0) ? {input_vars[0]} : 0.0f;")
            else:
                ops = {Operators.ADD: '+', Operators.SUB: '-', Operators.MUL: '*', Operators.DIV: '/'}
                self.code_buffer.append(f"        float {var_name} = {input_vars[0]} {ops[node.op_type]} {input_vars[1]};")
            
            self.code_buffer.append(f"        vars[{idx}][i] = {var_name};")

    
    def compile_backward(self, root_node: Node) -> str: 
        self.visited_nodes.clear()
        self.execution_order.clear()
        self._topological_sort(root_node)
        self.code_buffer.clear()
        self.code_buffer = [
            "#include <arm_neon.h>", 
            "#include <omp.h>", 
            "#include <vector>",
            "#include <cmath>",  
            "", 
            "// grads[i] maps to execution_order[i] gradient buffer", 
            "extern \"C\" void compute_backward(float** grads, float** inputs, int n, float* output_grad){", 
            "   int vector_limit = n-(n%4);",
            "   #pragma omp parallel for",
        ]
        self.code_buffer.append("   for (int i = 0; i<vector_limit; i+=4){")
        root_idx = len(self.execution_order)-1
        self.code_buffer.append(f"      float32x4_t v_grad_out = vld1q_f32(&output_grad[i]);")
        # Emit code in REVERSE topological order
        for node in reversed(self.execution_order):
            self._emit_backward_vector_node_code(node)

        self.code_buffer.append("   }")

        # scalar Tail Loop
        self.code_buffer.append("      for (int i=vector_limit; i < n; ++i){")
        self.code_buffer.append("          float s_grad_out = output_grad[i]; ")
        for node in reversed(self.execution_order): 
            self._emit_backward_node_code(node)
        self.code_buffer.append("   }")
        self.code_buffer.append("}")

        return "\n".join(self.code_buffer)
    
    def _emit_backward_vector_node_code(self, node: Node) -> None: 
        node_idx = self.execution_order.index(node)
        if node.op_type == Operators.ADD:
            # grad(left) += grad_out ; grad(right) += grad_out
            for input_node in node.inputs:
                in_idx = self.execution_order.index(input_node)
                self.code_buffer.append(f"      float32x4_t v_g_{in_idx} = vld1q_f32(&grads[{in_idx}][i]);")
                self.code_buffer.append(f"      v_g_{in_idx} = vaddq_f32(v_g_{in_idx}, v_grad_out);")
                self.code_buffer.append(f"      vst1q_f32(&grads[{in_idx}][i], v_g_{in_idx});")
        elif node.op_type == Operators.SUB:
            # left - right: grad(left)+=grad_out ; grad(right)-=grad_out
            left_node, right_node = node.inputs
            left_idx = self.execution_order.index(left_node)
            right_idx = self.execution_order.index(right_node)
            self.code_buffer.append(f"      float32x4_t v_g_{left_idx} = vld1q_f32(&grads[{left_idx}][i]);")
            self.code_buffer.append(f"      v_g_{left_idx} = vaddq_f32(v_g_{left_idx}, v_grad_out);")
            self.code_buffer.append(f"      vst1q_f32(&grads[{left_idx}][i], v_g_{left_idx});")
            self.code_buffer.append(f"      float32x4_t v_g_{right_idx} = vld1q_f32(&grads[{right_idx}][i]);")
            self.code_buffer.append(f"      v_g_{right_idx} = vsubq_f32(v_g_{right_idx}, v_grad_out);")
            self.code_buffer.append(f"      vst1q_f32(&grads[{right_idx}][i], v_g_{right_idx});")

        elif node.op_type == Operators.MUL:
            # grad_a += grad_out * b ; grad_b += grad_out * a
            a_node, b_node = node.inputs
            a_idx, b_idx = self.execution_order.index(a_node), self.execution_order.index(b_node)
            self.code_buffer.append(f"      float32x4_t v_act_b = vld1q_f32(&inputs[{b_idx}][i]);")
            self.code_buffer.append(f"      float32x4_t v_g_a = vld1q_f32(&grads[{a_idx}][i]);")
            self.code_buffer.append(f"      v_g_a = vfmaq_f32(v_g_a, v_grad_out, v_act_b);")
            self.code_buffer.append(f"      vst1q_f32(&grads[{a_idx}][i], v_g_a);")
            self.code_buffer.append(f"      float32x4_t v_act_a = vld1q_f32(&inputs[{a_idx}][i]);")
            self.code_buffer.append(f"      float32x4_t v_g_b = vld1q_f32(&grads[{b_idx}][i]);")
            self.code_buffer.append(f"      v_g_b = vfmaq_f32(v_g_b, v_grad_out, v_act_a);")
            self.code_buffer.append(f"      vst1q_f32(&grads[{b_idx}][i], v_g_b);")

        elif node.op_type == Operators.DIV:
            # a / b: grad_a += grad_out / b ; grad_b -= grad_out * a / (b*b)
            a_node, b_node = node.inputs
            a_idx, b_idx = self.execution_order.index(a_node), self.execution_order.index(b_node)
            self.code_buffer.append(f"      float32x4_t v_act_a = vld1q_f32(&inputs[{a_idx}][i]);")
            self.code_buffer.append(f"      float32x4_t v_act_b = vld1q_f32(&inputs[{b_idx}][i]);")
            self.code_buffer.append(f"      float32x4_t v_g_a = vld1q_f32(&grads[{a_idx}][i]);")
            self.code_buffer.append(f"      float32x4_t v_g_b = vld1q_f32(&grads[{b_idx}][i]);")
            # v_g_a += v_grad_out / v_act_b
            self.code_buffer.append(f"      float32x4_t v_tmp = vdivq_f32(v_grad_out, v_act_b);")
            self.code_buffer.append(f"      v_g_a = vaddq_f32(v_g_a, v_tmp);")
            self.code_buffer.append(f"      vst1q_f32(&grads[{a_idx}][i], v_g_a);")
            # v_g_b -= v_grad_out * v_act_a / (v_act_b * v_act_b)
            self.code_buffer.append(f"      float32x4_t v_b2 = vmulq_f32(v_act_b, v_act_b);")
            self.code_buffer.append(f"      float32x4_t v_tmp2 = vdivq_f32(v_act_a, v_b2);")
            self.code_buffer.append(f"      v_tmp2 = vmulq_f32(v_grad_out, v_tmp2);")
            self.code_buffer.append(f"      v_g_b = vsubq_f32(v_g_b, v_tmp2);")
            self.code_buffer.append(f"      vst1q_f32(&grads[{b_idx}][i], v_g_b);")

            
        elif node.op_type == Operators.RELU:
            input_node = node.inputs[0]
            in_idx = self.execution_order.index(input_node)
            # 1. Load data
            self.code_buffer.append(f"      float32x4_t v_act_in = vld1q_f32(&inputs[{in_idx}][i]);")
            self.code_buffer.append(f"      float32x4_t v_g_in = vld1q_f32(&grads[{in_idx}][i]);")
            self.code_buffer.append(f"      float32x4_t v_zero = vdupq_n_f32(0.0f);")
            
            # 2. Create Mask (x > 0)
            self.code_buffer.append(f"      uint32x4_t mask = vcgtq_f32(v_act_in, v_zero);")
            
            # 3. Bitwise AND (Corrected casting for NEON)
            self.code_buffer.append(f"      uint32x4_t v_g_out_u32 = vreinterpretq_u32_f32(v_grad_out);")
            self.code_buffer.append(f"      float32x4_t v_grad_masked = vreinterpretq_f32_u32(vandq_u32(v_g_out_u32, mask));")
            
            # 4. Accumulate
            self.code_buffer.append(f"      v_g_in = vaddq_f32(v_g_in, v_grad_masked);")
            self.code_buffer.append(f"      vst1q_f32(&grads[{in_idx}][i], v_g_in);")

    def _emit_backward_node_code(self, node: Node) -> None:
        node_idx = self.execution_order.index(node)
        if node.op_type == Operators.ADD:
            for input_node in node.inputs: 
                in_idx = self.execution_order.index(input_node)
                self.code_buffer.append(f"          grads[{in_idx}][i] += s_grad_out;")
        elif node.op_type == Operators.SUB: 
            left_node, right_node = node.inputs
            left_idx = self.execution_order.index(left_node)
            right_idx = self.execution_order.index(right_node)
            self.code_buffer.append(f"          grads[{left_idx}][i] += s_grad_out;")
            self.code_buffer.append(f"          grads[{right_idx}][i] -= s_grad_out;")

        elif node.op_type == Operators.MUL: 
            a_node, b_node = node.inputs
            a_idx, b_idx = self.execution_order.index(a_node), self.execution_order.index(b_node)
            self.code_buffer.append(f"          grads[{a_idx}][i] += s_grad_out * inputs[{b_idx}][i];")
            self.code_buffer.append(f"          grads[{b_idx}][i] += s_grad_out * inputs[{a_idx}][i];")
        
        elif node.op_type == Operators.DIV: 
            a_node, b_node = node.inputs
            a_idx, b_idx = self.execution_order.index(a_node), self.execution_order.index(b_node)
            self.code_buffer.append(f"          grads[{a_idx}][i] += s_grad_out / inputs[{b_idx}][i];")
            self.code_buffer.append(f"          grads[{b_idx}][i] -= s_grad_out * inputs[{a_idx}][i] / (inputs[{b_idx}][i] * inputs[{b_idx}][i]);")
        elif node.op_type == Operators.RELU:
            input_node = node.inputs[0]
            in_idx = self.execution_order.index(input_node)
            self.code_buffer.append(f"          if (inputs[{in_idx}][i] > 0)  grads[{in_idx}][i] += s_grad_out;")

    def compile_matmul_backward(self, node: Node) -> str: 
        input_a, input_b = node.inputs
        M,K = input_a.shape
        _, N = input_b.shape

        code = [
                "// MatMul Backward Kernel generated by VectorJIT",
                "extern \"C\" void compute_backward(float** grads, float** inputs, int n, float* grad_out) {",
                f"    float* grad_a = grads[0]; float* grad_b = grads[1];",
                f"    float* A = inputs[0]; float* B = inputs[1];",
                "",
                "    // 1. grad_A = grad_out @ B.T (Shape: M x K) [cite: 80]",
                "    #pragma omp parallel for collapse(2)",
                f"    for (int i = 0; i < {M}; ++i) {{",
                f"        for (int j = 0; j < {K}; ++j) {{",
                "            float sum = 0.0f;",
                f"            for (int l = 0; l < {N}; ++l) {{",
                f"                sum += grad_out[i * {N} + l] * B[j * {N} + l];",
                "            }",
                f"            grad_a[i * {K} + j] += sum;",
                "        }",
                "    }",
                "",
                "    // 2. grad_B = A.T @ grad_out (Shape: K x N) ",
                "    #pragma omp parallel for collapse(2)",
                f"    for (int i = 0; i < {K}; ++i) {{",
                f"        for (int j = 0; j < {N}; ++j) {{",
                "            float sum = 0.0f;",
                f"            for (int l = 0; l < {M}; ++l) {{",
                f"                sum += A[l * {K} + i] * grad_out[l * {N} + j];",
                "            }",
                f"            grad_b[i * {N} + j] += sum;",
                "        }",
                "    }",
                "}" 
        ]
        return "\n".join(code)
    
    def compile_LayerNorm_Backward(self, node: Node)-> str: 
        eps = node.attributes.get('eps', 1e-5)
        # Assuming shape is (Batch, Features)
        B, F = node.shape 
        
        code = [
            "// Fused LayerNorm Backward Kernel",
            "extern \"C\" void layernorm_backward(float* grad_x, float* grad_gamma, float* grad_beta, "
            "                                    float* grad_out, float* x, float* gamma, int B, int F) {",
            "    #pragma omp parallel for",
            f"    for (int i = 0; i < B; ++i) {{",
            "        float sum_grad = 0.0f;",
            "        float sum_grad_xhat = 0.0f;",
            "        float mean = 0.0f;",
            "        float var = 0.0f;",
            "",
            "        // Pass 1: Calculate Mean and Variance for this row",
            "        for (int j = 0; j < F; ++j) { mean += x[i*F + j]; }",
            "        mean /= F;",
            "        for (int j = 0; j < F; ++j) { ",
            "            float diff = x[i*F + j] - mean;",
            "            var += diff * diff;",
            "        }",
            f"        float inv_std = 1.0f / sqrtf((var / F) + {eps}f);",
            "",
            "        // Pass 2: Calculate gradients for gamma, beta, and intermediate input grad",
            "        for (int j = 0; j < F; ++j) {",
            "            float x_hat = (x[i*F + j] - mean) * inv_std;",
            "            float g_out = grad_out[i*F + j];",
            "            grad_gamma[j] += g_out * x_hat; // Accumulate gamma grad",
            "            grad_beta[j] += g_out;         // Accumulate beta grad",
            "            sum_grad += g_out;",
            "            sum_grad_xhat += g_out * x_hat;",
            "        }",
            "",
            "        // Pass 3: Final input gradient dL/dx using the 5-term formula",
            "        for (int j = 0; j < F; ++j) {",
            "            float x_hat = (x[i*F + j] - mean) * inv_std;",
            "            grad_x[i*F + j] += (gamma[j] * inv_std / F) * ",
            "                               (F * grad_out[i*F + j] - sum_grad - x_hat * sum_grad_xhat);",
            "        }",
            "    }",
            "}"
        ]
        return "\n".join(code)
    
    def compile_LayerNorm_SIMD(self, node: Node) -> str: 
        eps = node.attributes.get("eps", 1e-5)
        B, F = node.shape
        code = [
            "#include <arm_neon.h>", 
            "#include <cmath>", 
            "#include <omp.h>", 
            "", 
            "extern \"C\" void compute(float** vars, int n){", 
            "   float *output = vars[0];",
            "   float *x = vars[1]; float* gamma = vars[2]; float* beta = vars[3];", 
            "   int vector_limit = " + str(F - (F%4)) + ";", 
            "", 
            "   #pragma omp parallel for", 
            f"  for(int i =0; i < {B}; ++i){{", 
            "       float sum = 0.0f; float sum_sq  = 0.0f;", 
            "       for(int j=0; j< " + str(F) + "; ++j){", 
            f"          float val = x[i * {F} + j];", 
            "           sum += val; sum_sq += val*val;", 
            "       }", 
            f"      float mean = sum/{F};",
            f"      float var = (sum_sq/{F})-(mean*mean);", 
            f"      float inv_std=1.0f/sqrt(var+{eps}f);", 
            "       float32x4_t v_mean = vdupq_n_f32(mean);",
            "       float32x4_t v_inv_std = vdupq_n_f32(inv_std);", 
            "       int j=0;",
            "       for (; j< vector_limit; j+=4){", 
            f"           int idx = i * {F} + j;", 
            "            float32x4_t v_x = vld1q_f32(&x[idx]);",
            "            float32x4_t v_gamma = vld1q_f32(&gamma[j]);", 
            "            float32x4_t v_beta = vld1q_f32(&beta[j]);", 
            "            float32x4_t v_x_hat = vmulq_f32(vsubq_f32(v_x, v_mean), v_inv_std);", 
            "            float32x4_t v_out = vfmaq_f32(v_beta, v_x_hat, v_gamma);", 
            "            vst1q_f32(&output[idx], v_out);", 
            "       }", 
            "       // Scalar Tail for non-multiples of 4", 
            "       for(; j < "+str(F)+"; ++j){", 
            f"           int idx = i * {F} + j; ", 
            "            output[idx] = ((x[idx] - mean) * inv_std) * gamma[j] + beta[j];", 
            "        }",
            "    }",
            "}"      
        ]
        return "\n".join(code)
    
    def compile_LayerNorm_Backend_SIMD(self, node: Node)-> str: 
        eps = node.attributes.get('eps', 1e-5)
        B, F = node.shape
        vector_limit = F - (F % 4)

        code = [
            "#include <arm_neon.h>", 
            "#include <cmath>", 
            "#include <omp.h>", 
            "", 
            "extern \"C\" void layernorm_backward(float* grad_x, float* grad_gamma, float* grad_beta, float* grad_out, float* x, float* gamma, int B, int F){", 
            "", 
            "   #pragma omp parallel for", 
            "   for (int i =0; i < " + str(B)+"; ++i ){", 
            "       float row_sum_grad = 0.0f;", 
            "       float row_sum_grad_xhat = 0.0f;",
            "       float row_sum_x = 0.0f; float row_sum_xsq = 0.0f;", 
            "       int offset = i * F;",
            "", 
            "       for (int j = 0; j < " + str(F) + "; ++j){", 
            "           float val = x[offset + j];", 
            "           row_sum_x += val; row_sum_xsq +=val*val;", 
            "       }", 
            f"      float mean = row_sum_x/{F};", 
            f"      float var = (row_sum_xsq/{F})-(mean*mean);", 
            f"      float inv_std = 1.0f / sqrtf(var + {eps}f);", 
            "", 
            "       float32x4_t  v_sum_grad = vdupq_n_f32(0.0f);", 
            "       float32x4_t  v_sum_grad_xhat = vdupq_n_f32(0.0f);", 
            "       float32x4_t  v_mean = vdupq_n_f32(mean);", 
            "       float32x4_t v_inv_std = vdupq_n_f32(inv_std);", 
            "", 
            "       int j =0; ", 
            "       for(; j < "+str(vector_limit)+"; j +=4){", 
            "           float32x4_t v_x = vld1q_f32(&x[offset + j]);",
            "           float32x4_t v_g_out = vld1q_f32(&grad_out[offset + j]);", 
            "           float32x4_t v_x_hat = vmulq_f32(vsubq_f32(v_x, v_mean), v_inv_std);", 
            "           v_sum_grad = vaddq_f32(v_sum_grad, v_g_out);", 
            "           v_sum_grad_xhat = vfmaq_f32(v_sum_grad_xhat, v_g_out, v_x_hat);", 
            "       }", 
            "       row_sum_grad = vaddvq_f32(v_sum_grad);", 
            "       row_sum_grad_xhat = vaddvq_f32(v_sum_grad_xhat);",
            "       float32x4_t v_s_grad = vdupq_n_f32(row_sum_grad);", 
            "       float32x4_t v_s_grad_xhat = vdupq_n_f32(row_sum_grad_xhat);", 
            f"      float32x4_t v_F_inv = vdupq_n_f32(1.0f/{F});", 
            "       j = 0;", 
            f"      for (; j < {vector_limit}; j += 4) {{",
            "            float32x4_t v_x = vld1q_f32(&x[offset + j]);",
            "            float32x4_t v_g_out = vld1q_f32(&grad_out[offset + j]);",
            "            float32x4_t v_gamma = vld1q_f32(&gamma[j]);",
            "            float32x4_t v_x_hat = vmulq_f32(vsubq_f32(v_x, v_mean), v_inv_std);",
            "",
            "            // Formula: (gamma * inv_std / F) * (F * grad_out - sum_grad - x_hat * sum_grad_xhat)",
            f"            float32x4_t v_term1 = vmulq_f32(vmulq_f32(v_gamma, v_inv_std), v_F_inv);",
            f"            float32x4_t v_term2 = vsubq_f32(vsubq_f32(vmulq_f32(vdupq_n_f32({F}), v_g_out), v_s_grad), vmulq_f32(v_x_hat, v_s_grad_xhat));",
            "            vst1q_f32(&grad_x[offset + j], vmulq_f32(v_term1, v_term2));",
            "      }", 
            "   }",
            "}"
        ]
        return "\n".join(code)

    def compile_fused_loss_forward(self, node: Node) -> str:
        B, F = node.inputs[0].shape # Logits shape
        code = [
            "#include <arm_neon.h>", "#include <cmath>", "#include <omp.h>",
            "extern \"C\" void compute(float** vars, int n) {",
            "    float* loss_out = vars[0]; float* logits = vars[1]; float* targets = vars[2];",
            "    #pragma omp parallel for",
            f"    for (int i = 0; i < {B}; ++i) {{",
            f"        int offset = i * {F};",
            "        float max_l = logits[offset];",
            f"        for (int j = 1; j < {F}; ++j) if (logits[offset+j] > max_l) max_l = logits[offset+j];",
            "        float sum_e = 0.0f;",
            f"        for (int j = 0; j < {F}; ++j) sum_e += expf(logits[offset+j] - max_l);",
            "        int target_idx = (int)targets[i];",
            "        loss_out[i] = (max_l + logf(sum_e)) - logits[offset + target_idx];",
            "    }",
            "}"
        ]
        return "\n".join(code)
    
    def compile_fused_loss_backward(self, node: Node) -> str:
        # We assume node.inputs[0] is Logits (B, F) and node.inputs[1] is Targets (B)
        B, F = node.inputs[0].shape
        code = [
            "#include <arm_neon.h>",
            "#include <cmath>",
            "#include <omp.h>",
            "",
            "extern \"C\" void loss_backward(float** grads, float** inputs, int B, float* grad_out) {",
            "    float* grad_logits = grads[0];",
            "    float* logits = inputs[0];",
            "    float* targets = inputs[1];",
            f"    int F = {F};",
            "",
            "    #pragma omp parallel for",
            "    for (int i = 0; i < B; ++i) {",
            "        int offset = i * F;",
            "        // 1. Recompute Softmax for this row",
            "        float max_l = logits[offset];",
            "        for (int j = 1; j < F; ++j) { if(logits[offset+j] > max_l) max_l = logits[offset+j]; }",
            "        ",
            "        float sum_e = 0.0f;",
            "        for (int j = 0; j < F; ++j) { sum_e += expf(logits[offset+j] - max_l); }",
            "",
            "        // 2. Gradient: (prob - 1) if j == target else prob",
            "        int target_idx = (int)targets[i];",
            "        for (int j = 0; j < F; ++j) {",
            "            float prob = expf(logits[offset+j] - max_l) / sum_e;",
            "            // Multiply by grad_out[i] in case of weighted loss, though usually 1.0",
            "            grad_logits[offset + j] = (j == target_idx) ? (prob - 1.0f) : prob;",
            "            grad_logits[offset + j] *= grad_out[i];",
            "        }",
            "    }",
            "}"
        ]
        return "\n".join(code)
    
    def jit_compile(self, root_node: Node) -> ctypes.CDLL:
        # 1. Generate Source Code
        key = self._get_cache_key(root_node, is_backward=False)
        if key in Compiler._binary_cache:
            if root_node.op_type not in (Operators.MATMUL, Operators.LAYERNORM, Operators.SOFTMAX_CROSS_ENTROPY):
                # Rebuild execution_order and node_idx_map for this graph
                self.visited_nodes.clear()
                self.execution_order.clear()
                self._topological_sort(root_node)
                self.node_idx_map = {node.id: i for i, node in enumerate(self.execution_order)}
            return Compiler._binary_cache[key]

        if root_node.op_type == Operators.MATMUL:
            cpp_code = self.compile_matMul_SIMD(root_node)
        elif root_node.op_type == Operators.LAYERNORM:
            cpp_code = self.compile_LayerNorm_SIMD(root_node)
        elif root_node.op_type == Operators.SOFTMAX_CROSS_ENTROPY:
            cpp_code = self.compile_fused_loss_forward(root_node)
        else:
            cpp_code = self.compile(root_node)
        
        # 2. Write to Temp File
        with tempfile.NamedTemporaryFile(suffix=".cpp", delete=False) as f:
            src_path = f.name
            f.write(cpp_code.encode('utf-8'))
            
        lib_path = src_path.replace(".cpp", ".so")
        
        # 3. Build Compilation Command
        compile_cmd = ["c++", "-O3", "-shared", "-fPIC", "-march=native"]
        
        # macOS Detection for OpenMP
        import platform
        if platform.system() == "Darwin":
            # Apple Clang needs special flags for OpenMP
            compile_cmd += ["-Xpreprocessor", "-fopenmp"]
            compile_cmd += ["-lomp"]
            
            # Add Homebrew paths for libomp (Critical for Apple Silicon)
            compile_cmd += ["-I/opt/homebrew/opt/libomp/include"]
            compile_cmd += ["-L/opt/homebrew/opt/libomp/lib"]
        else:
            # Linux/Windows (GCC)
            compile_cmd += ["-fopenmp"]
            
        compile_cmd += [src_path, "-o", lib_path]
        
        # 4. Run Compiler & Capture Output
        result = subprocess.run(compile_cmd, capture_output=True, text=True)
        if result.returncode != 0: 
            print("Compilation failed:")
            print(result.stderr)
            raise RuntimeError("C++ compilation failed")


        # 6. Load Library
        try:
            lib = ctypes.CDLL(lib_path)
        except OSError as e:
            print(f"Failed to load library at {lib_path}")
            raise e
        
        lib.compute.argtypes = [
            ctypes.POINTER(ctypes.POINTER(ctypes.c_float)), 
            ctypes.c_int
        ]

        Compiler._binary_cache[key] = lib
        os.remove(src_path)
        return lib
    
    def jit_compile_backward(self, root_node: Node) -> tuple:

        key = self._get_cache_key(root_node, is_backward=True)
        if key in Compiler._binary_cache:
            # For generic backward, rebuild execution_order
            if root_node.op_type not in (Operators.MATMUL, Operators.LAYERNORM, Operators.SOFTMAX_CROSS_ENTROPY):
                self.visited_nodes.clear()
                self.execution_order.clear()
                self._topological_sort(root_node)
            lib, func_name = Compiler._binary_cache[key]
            return lib, func_name

        # 2. Dispatch to the correct Generator
        if root_node.op_type == Operators.MATMUL:
            cpp_code = self.compile_matmul_backward(root_node)
            func_name = "compute_backward"
        elif root_node.op_type == Operators.LAYERNORM:
            cpp_code = self.compile_LayerNorm_Backend_SIMD(root_node)
            func_name = "layernorm_backward"
        elif root_node.op_type == Operators.SOFTMAX_CROSS_ENTROPY:
            cpp_code = self.compile_fused_loss_backward(root_node)
            func_name = "loss_backward"
        else:
            cpp_code = self.compile_backward(root_node)
            func_name = "compute_backward"

        # 2. Write to Temp File
        with tempfile.NamedTemporaryFile(suffix=".cpp", delete=False) as f:
            src_path = f.name
            f.write(cpp_code.encode('utf-8'))

        lib_path = src_path.replace(".cpp", ".so")
        
        # 3. Build Compilation Command
        compile_cmd = ["c++", "-O3", "-shared", "-fPIC", "-march=native"]
        
        # macOS Detection for OpenMP
        import platform
        if platform.system() == "Darwin":
            # Apple Clang needs special flags for OpenMP
            compile_cmd += ["-Xpreprocessor", "-fopenmp"]
            compile_cmd += ["-lomp"]
            
            # Add Homebrew paths for libomp (Critical for Apple Silicon)
            compile_cmd += ["-I/opt/homebrew/opt/libomp/include"]
            compile_cmd += ["-L/opt/homebrew/opt/libomp/lib"]
        else:
            # Linux/Windows (GCC)
            compile_cmd += ["-fopenmp"]
            
        compile_cmd += [src_path, "-o", lib_path]
        
        # 4. Run Compiler & Capture Output
        result = subprocess.run(compile_cmd, capture_output=True, text=True)
        if result.returncode != 0: 
            print("Compilation failed:")
            print(result.stderr)
            raise RuntimeError("C++ compilation failed")
        
        try:
            lib = ctypes.CDLL(lib_path)
        except OSError as e:
            print(f"Failed to load library at {lib_path}")
            raise e
        

        func = getattr(lib, func_name)
        
        if root_node.op_type == Operators.LAYERNORM:
             func.argtypes = [
                ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_float), # grad_x, grad_gamma
                ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_float), # grad_beta, grad_out
                ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_float), # x, gamma
                ctypes.c_int, ctypes.c_int # B, F
            ]
        else:
            # Unified signature for Loss, MatMul, and Element-wise chains
            func.argtypes = [
                ctypes.POINTER(ctypes.POINTER(ctypes.c_float)), # grads array
                ctypes.POINTER(ctypes.POINTER(ctypes.c_float)), # inputs array
                ctypes.c_int,                                   # Batch/Size
                ctypes.POINTER(ctypes.c_float)                  # grad_out
            ]

        # 5. Save to Cache
        Compiler._binary_cache[key] = (lib, func_name)
        os.remove(src_path)
        return lib, func_name