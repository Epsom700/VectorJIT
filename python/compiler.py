import code
from platform import node
from .ops import Node, Operators
from typing import List, Optional, Set, Dict
import subprocess
import ctypes
import numpy as np
import os
import tempfile


import atexit

_loaded_libraries = []  # Module-level list that prevents GC

def _cleanup():
    """Prevent segfault by keeping library references alive during shutdown."""
    _loaded_libraries.clear()

atexit.register(_cleanup)
class Compiler: 
    _binary_cache: Dict[str, ctypes.CDLL] = {}
    def __init__(self): 
        self.visited_nodes: Set[int] = set()
        self.execution_order: List[Node] = []
        self.code_buffer: List[str] = []
        self.var_map: Dict[int, str] = {}
        self.temp_file: List[str] = []
        
    def _get_cache_key(self, node: Node, is_backward: bool = False) -> str:
        """Generates a unique key based on the node structure and shapes."""
        input_shapes = tuple(n.shape for n in node.inputs)
        # Include attributes (like eps in LayerNorm) in the hash
        attr_str = str(sorted(node.attributes.items()))
        return f"{node.op_type.name}_{node.shape}_{input_shapes}_{attr_str}_back={is_backward}"

    def _topological_sort(self, node: Node) -> List[Node]:
        if node.id in self.visited_nodes: 
            return 
        
        self.visited_nodes.add(node.id)

        is_fused = node.op_type in (Operators.MATMUL, Operators.LAYERNORM, Operators.SOFTMAX_CROSS_ENTROPY)
        
        if not is_fused:
            for input_node in node.inputs: 
                self._topological_sort(input_node)

        self.execution_order.append(node)
        return self.execution_order  # Return for convenience

    def compile(self, root_node: Node) -> str: 
        self.visited_nodes.clear()
        self.execution_order.clear()
        self.code_buffer.clear()
        self._topological_sort(root_node)
        
        # Create map for O(1) index lookup
        self.node_idx_map = {node.id: i for i, node in enumerate(self.execution_order)}

        if root_node.op_type == Operators.MATMUL: 
            return self.compile_matMul_SIMD(root_node)
        
        if root_node.op_type == Operators.LAYERNORM: 
            return self.compile_LayerNorm_SIMD(root_node)
        
        if root_node.op_type == Operators.SOFTMAX_CROSS_ENTROPY:
            return self.compile_fused_loss_forward(root_node)
        
        # 1. Header & Shared State
        self.code_buffer = [
            "#include <iostream>",
            "#include <vector>",
            "#include <cmath>",
            "#include <arm_neon.h>",
            "#include <omp.h>", 
            "",
            "",
            "// Generated by VectorJIT",
            "extern \"C\" void compute(float** vars, int n) {",
            "    int vector_limit = n - (n % 4);", 
            "    float32x4_t zero_vec = vdupq_n_f32(0.0f);",
            "    #pragma omp parallel for"
            ""
        ]

        # 2. VECTOR LOOP
        # Use <= to ensure we process exact multiples of 4
        self.var_map.clear()
        self.code_buffer.append("    for (int i = 0; i < vector_limit; i += 4) {")
        
        for node in self.execution_order: 
            self._emit_vector_node_code(node)

        self.code_buffer.append("    }")

        # 3. SCALAR TAIL LOOP
        self.var_map.clear()
        self.code_buffer.append("    // Scalar Cleanup")
        self.code_buffer.append("    for (int i = vector_limit; i < n; ++i) {")
        
        # We must re-emit scalar code because vector variables (float32x4_t) 
        # are not compatible with scalar math.
        for node in self.execution_order: 
            self._emit_node_code(node)
        
        self.code_buffer.append("    }")
        self.code_buffer.append("}")
        
        return "\n".join(self.code_buffer)
    
    def compile_matmul(self, node: Node) -> str:
        input_a = node.inputs[0]
        input_b = node.inputs[1]
        
        M = input_a.shape[0]
        K = input_a.shape[1]
        N = input_b.shape[1]
        
        # Resolve Input Indices
        idx_a = 0 if input_a.name == "A" else 1
        idx_b = 1 if idx_a == 0 else 0
        
        # BLOCK_SIZE: 32 floats * 32 floats * 4 bytes = 4KB.
        # This fits easily into L1 Cache (typically 32KB - 64KB).
        BLOCK_SIZE = 32

        code = [
            "#include <iostream>",
            "#include <omp.h>",
            "#include <algorithm>", # Required for std::min, std::fill_n
            "",
            "extern \"C\" void compute(float* output, float** inputs, int n) {",
            f"    const int M = {M};",
            f"    const int K = {K};",
            f"    const int N = {N};",
            f"    const int BLOCK_SIZE = {BLOCK_SIZE};",
            "    ",
            "    float* A = inputs[" + str(idx_a) + "];",
            "    float* B = inputs[" + str(idx_b) + "];",
            "    ",
            "    // 1. Initialize Output to Zero (Critical for accumulation)",
            "    // We use std::fill_n which is often optimized to memset",
            "    std::fill_n(output, M * N, 0.0f);",
            "    ",
            "    // 2. Tiled Matrix Multiplication",
            "    // Collapse the outer two loops to parallelize blocks across threads",
            "    #pragma omp parallel for collapse(2)",
            "    for (int ii = 0; ii < M; ii += BLOCK_SIZE) {",
            "        for (int jj = 0; jj < N; jj += BLOCK_SIZE) {",
            "            for (int kk = 0; kk < K; kk += BLOCK_SIZE) {",
            "                ",
            "                // 3. Inner Loops (Process one Block)",
            "                // Use std::min to handle boundary conditions",
            "                for (int i = ii; i < std::min(ii + BLOCK_SIZE, M); ++i) {",
            "                    for (int j = jj; j < std::min(jj + BLOCK_SIZE, N); ++j) {",
            "                        ",
            "                        // Load current value (if we were accumulating into output directly)",
            "                        // Use a temp 'current_val' for the inner K loop",
            "                        float current_val = output[i * N + j];",
            "                        ",
            "                        for (int k = kk; k < std::min(kk + BLOCK_SIZE, K); ++k) {",
            "                            current_val += A[i * K + k] * B[k * N + j];",
            "                        }",
            "                        output[i * N + j] = current_val;",
            "                    }",
            "                }",
            "            }",
            "        }",
            "    }",
            "}"
        ]
        return "\n".join(code)
    
    def compile_matMul_SIMD(self, node: Node) -> str: 
        input_a = node.inputs[0]
        input_b = node.inputs[1]
        M = input_a.shape[0]
        K = input_a.shape[1]
        N = input_b.shape[1]
        
        # Resolve Input Indices
        idx_a = 0 if input_a.name == "A" else 1
        idx_b = 1 if idx_a == 0 else 0
        
        # BLOCK_SIZE: 32 floats * 32 floats * 4 bytes = 4KB.
        # This fits easily into L1 Cache (typically 32KB - 64KB).
        BLOCK_SIZE = 32

        code = [
            "#include <iostream>",
            "#include <arm_neon.h>",
            "#include <omp.h>",
            "#include <algorithm>", # Required for std::min, std::fill_n
            "",
            "extern \"C\" void compute(float** vars, int n) {",
            f"    const int M = {M};",
            f"    const int N = {N}; ", 
            f"    const int K = {K};", 
            f"    const int BLOCK_SIZE = {BLOCK_SIZE};", 
            "   ", 
            "   ", 
            "   float* output = vars[0];",
            "   float* A = vars[1];", 
            "   float* B = vars[2]; ", 
            "   std::fill_n(output, M*N, 0.0f);", 
            "   #pragma omp parallel for collapse(2)", 
            "   for(int I = 0; I < M; I+=BLOCK_SIZE){",
            "       for (int J = 0; J < N; J+=BLOCK_SIZE){", 
            "           for (int L = 0; L < K;  L+=BLOCK_SIZE){", 
            "               int i_limit = std::min(I + BLOCK_SIZE, M);", 
            "               int j_limit = std::min(J + BLOCK_SIZE, N);", 
            "               int k_limit = std::min(L + BLOCK_SIZE, K); ", 
            "               for (int i = I; i< i_limit; ++i){", 
            "                   for(int k=L; k<k_limit; ++k){", 
            "                       float32x4_t vec_a = vdupq_n_f32(A[i * K + k]);", 
            "                       int j = J;", 
            "                       for (; j <= j_limit -4 ; j+=4){", 
            "                           float32x4_t vec_c = vld1q_f32(&output[i * N + j ]);", 
            "                           float32x4_t vec_b = vld1q_f32(&B[k*N+j]);", 
            "                           vec_c = vfmaq_f32(vec_c, vec_a, vec_b);", 
            "                           vst1q_f32(&output[i*N+j], vec_c);", 
            "                       }",
            "                       for (; j < j_limit; ++j){", 
            "                           output[i * N + j]+=A[i * K + k] * B[k * N + j];", 
            "                       }", 
            "                   }", 
            "               }", 
            "           }", 
            "       }", 
            "   }", 
            "}"

        ]

        return "\n".join(code)

    def _emit_vector_node_code(self, node: Node) -> None: 
        var_name = f"vec_{node.id}"
        idx = self.node_idx_map[node.id]

        if node.op_type == Operators.INPUT: 
            self.var_map[node.id] = var_name # Only register upon emission
            self.code_buffer.append(f"        float32x4_t {var_name} = vld1q_f32(&vars[{idx}][i]);")
            
        elif node.op_type == Operators.RELU:
            input_node = node.inputs[0]
            input_var = self.var_map.get(input_node.id)
            if not input_var:
                in_idx = self.node_idx_map[input_node.id]
                input_var = f"vec_pre_{input_node.id}"
                self.code_buffer.append(f"        float32x4_t {input_var} = vld1q_f32(&vars[{in_idx}][i]);")
                
            self.var_map[node.id] = var_name
            self.code_buffer.append(f"        float32x4_t {var_name} = vmaxq_f32({input_var}, zero_vec);")
            self.code_buffer.append(f"        vst1q_f32(&vars[{idx}][i], {var_name});")

        elif node.op_type == Operators.ADD:
            left_node, right_node = node.inputs
            left_var = self.var_map.get(left_node.id)
            right_var = self.var_map.get(right_node.id)
            if not left_var:
                in_idx = self.node_idx_map[left_node.id]
                left_var = f"vec_pre_{left_node.id}"
                self.code_buffer.append(f"        float32x4_t {left_var} = vld1q_f32(&vars[{in_idx}][i]);")
            if not right_var:
                in_idx = self.node_idx_map[right_node.id]
                right_var = f"vec_pre_{right_node.id}"
                self.code_buffer.append(f"        float32x4_t {right_var} = vld1q_f32(&vars[{in_idx}][i]);")

            self.var_map[node.id] = var_name
            self.code_buffer.append(f"        float32x4_t {var_name} = vaddq_f32({left_var}, {right_var});")
            self.code_buffer.append(f"        vst1q_f32(&vars[{idx}][i], {var_name});")

        elif node.op_type == Operators.SUB:
            left_node, right_node = node.inputs
            left_var = self.var_map.get(left_node.id)
            right_var = self.var_map.get(right_node.id)
            if not left_var:
                in_idx = self.node_idx_map[left_node.id]
                left_var = f"vec_pre_{left_node.id}"
                self.code_buffer.append(f"        float32x4_t {left_var} = vld1q_f32(&vars[{in_idx}][i]);")
            if not right_var:
                in_idx = self.node_idx_map[right_node.id]
                right_var = f"vec_pre_{right_node.id}"
                self.code_buffer.append(f"        float32x4_t {right_var} = vld1q_f32(&vars[{in_idx}][i]);")

            self.var_map[node.id] = var_name
            self.code_buffer.append(f"        float32x4_t {var_name} = vsubq_f32({left_var}, {right_var});")
            self.code_buffer.append(f"        vst1q_f32(&vars[{idx}][i], {var_name});")
        elif node.op_type == Operators.MUL:
            left_node, right_node = node.inputs
            left_var = self.var_map.get(left_node.id)
            right_var = self.var_map.get(right_node.id)
            if not left_var:
                in_idx = self.node_idx_map[left_node.id]
                left_var = f"vec_pre_{left_node.id}"
                self.code_buffer.append(f"        float32x4_t {left_var} = vld1q_f32(&vars[{in_idx}][i]);")
            if not right_var:
                in_idx = self.node_idx_map[right_node.id]
                right_var = f"vec_pre_{right_node.id}"
                self.code_buffer.append(f"        float32x4_t {right_var} = vld1q_f32(&vars[{in_idx}][i]);")

            self.var_map[node.id] = var_name
            self.code_buffer.append(f"        float32x4_t {var_name} = vmulq_f32({left_var}, {right_var});")
            self.code_buffer.append(f"        vst1q_f32(&vars[{idx}][i], {var_name});")

        elif node.op_type == Operators.DIV:
            left_node, right_node = node.inputs
            left_var = self.var_map.get(left_node.id)
            right_var = self.var_map.get(right_node.id)
            if not left_var:
                in_idx = self.node_idx_map[left_node.id]
                left_var = f"vec_pre_{left_node.id}"
                self.code_buffer.append(f"        float32x4_t {left_var} = vld1q_f32(&vars[{in_idx}][i]);")
            if not right_var:
                in_idx = self.node_idx_map[right_node.id]
                right_var = f"vec_pre_{right_node.id}"
                self.code_buffer.append(f"        float32x4_t {right_var} = vld1q_f32(&vars[{in_idx}][i]);")

            self.var_map[node.id] = var_name
            self.code_buffer.append(f"        float32x4_t {var_name} = vdivq_f32({left_var}, {right_var});")
            self.code_buffer.append(f"        vst1q_f32(&vars[{idx}][i], {var_name});")

    def _emit_node_code(self, node: Node) -> None: 
        var_name = f"var_{node.id}"
        idx = self.node_idx_map[node.id]

        if node.op_type == Operators.INPUT:
            self.var_map[node.id] = var_name
            self.code_buffer.append(f"        float {var_name} = vars[{idx}][i];")
            
        elif node.op_type in (Operators.ADD, Operators.SUB, Operators.MUL, Operators.DIV, Operators.RELU):
            # Resolve inputs from map or memory
            input_vars = []
            for inp in node.inputs:
                if inp.id in self.var_map:
                    input_vars.append(self.var_map[inp.id])
                else:
                    in_idx = self.node_idx_map[inp.id]
                    tmp_v = f"var_pre_{inp.id}"
                    self.code_buffer.append(f"        float {tmp_v} = vars[{in_idx}][i];")
                    input_vars.append(tmp_v)

            self.var_map[node.id] = var_name
            if node.op_type == Operators.RELU:
                self.code_buffer.append(f"        float {var_name} = ({input_vars[0]} > 0) ? {input_vars[0]} : 0.0f;")
            else:
                ops = {Operators.ADD: '+', Operators.SUB: '-', Operators.MUL: '*', Operators.DIV: '/'}
                self.code_buffer.append(f"        float {var_name} = {input_vars[0]} {ops[node.op_type]} {input_vars[1]};")
            
            self.code_buffer.append(f"        vars[{idx}][i] = {var_name};")

    def compile_backward(self, root_node: Node) -> str: 
        self.visited_nodes.clear()
        self.execution_order.clear()
        self._topological_sort(root_node)
        self.code_buffer.clear()
        self.code_buffer = [
            "#include <arm_neon.h>", 
            "#include <omp.h>", 
            "#include <vector>",
            "#include <cmath>",  
            "", 
            "// grads[i] maps to execution_order[i] gradient buffer", 
            "extern \"C\" void compute_backward(float** grads, float** inputs, int n, float* output_grad){", 
            "   int vector_limit = n-(n%4);",
            "   #pragma omp parallel for",
        ]
        self.code_buffer.append("   for (int i = 0; i<vector_limit; i+=4){")
        root_idx = len(self.execution_order)-1
        self.code_buffer.append(f"      float32x4_t v_grad_out = vld1q_f32(&output_grad[i]);")
        # Emit code in REVERSE topological order
        for node in reversed(self.execution_order):
            self._emit_backward_vector_node_code(node)

        self.code_buffer.append("   }")

        # scalar Tail Loop
        self.code_buffer.append("      for (int i=vector_limit; i < n; ++i){")
        self.code_buffer.append("          float s_grad_out = output_grad[i]; ")
        for node in reversed(self.execution_order): 
            self._emit_backward_node_code(node)
        self.code_buffer.append("   }")
        self.code_buffer.append("}")

        return "\n".join(self.code_buffer)
    
    def _emit_backward_vector_node_code(self, node: Node) -> None: 
        node_idx = self.execution_order.index(node)
        if node.op_type == Operators.ADD:
            # grad(left) += grad_out ; grad(right) += grad_out
            for input_node in node.inputs:
                in_idx = self.execution_order.index(input_node)
                self.code_buffer.append(f"      float32x4_t v_g_{in_idx} = vld1q_f32(&grads[{in_idx}][i]);")
                self.code_buffer.append(f"      v_g_{in_idx} = vaddq_f32(v_g_{in_idx}, v_grad_out);")
                self.code_buffer.append(f"      vst1q_f32(&grads[{in_idx}][i], v_g_{in_idx});")
        elif node.op_type == Operators.SUB:
            # left - right: grad(left)+=grad_out ; grad(right)-=grad_out
            left_node, right_node = node.inputs
            left_idx = self.execution_order.index(left_node)
            right_idx = self.execution_order.index(right_node)
            self.code_buffer.append(f"      float32x4_t v_g_{left_idx} = vld1q_f32(&grads[{left_idx}][i]);")
            self.code_buffer.append(f"      v_g_{left_idx} = vaddq_f32(v_g_{left_idx}, v_grad_out);")
            self.code_buffer.append(f"      vst1q_f32(&grads[{left_idx}][i], v_g_{left_idx});")
            self.code_buffer.append(f"      float32x4_t v_g_{right_idx} = vld1q_f32(&grads[{right_idx}][i]);")
            self.code_buffer.append(f"      v_g_{right_idx} = vsubq_f32(v_g_{right_idx}, v_grad_out);")
            self.code_buffer.append(f"      vst1q_f32(&grads[{right_idx}][i], v_g_{right_idx});")

        elif node.op_type == Operators.MUL:
            # grad_a += grad_out * b ; grad_b += grad_out * a
            a_node, b_node = node.inputs
            a_idx, b_idx = self.execution_order.index(a_node), self.execution_order.index(b_node)
            self.code_buffer.append(f"      float32x4_t v_act_b = vld1q_f32(&inputs[{b_idx}][i]);")
            self.code_buffer.append(f"      float32x4_t v_g_a = vld1q_f32(&grads[{a_idx}][i]);")
            self.code_buffer.append(f"      v_g_a = vfmaq_f32(v_g_a, v_grad_out, v_act_b);")
            self.code_buffer.append(f"      vst1q_f32(&grads[{a_idx}][i], v_g_a);")
            self.code_buffer.append(f"      float32x4_t v_act_a = vld1q_f32(&inputs[{a_idx}][i]);")
            self.code_buffer.append(f"      float32x4_t v_g_b = vld1q_f32(&grads[{b_idx}][i]);")
            self.code_buffer.append(f"      v_g_b = vfmaq_f32(v_g_b, v_grad_out, v_act_a);")
            self.code_buffer.append(f"      vst1q_f32(&grads[{b_idx}][i], v_g_b);")

        elif node.op_type == Operators.DIV:
            # a / b: grad_a += grad_out / b ; grad_b -= grad_out * a / (b*b)
            a_node, b_node = node.inputs
            a_idx, b_idx = self.execution_order.index(a_node), self.execution_order.index(b_node)
            self.code_buffer.append(f"      float32x4_t v_act_a = vld1q_f32(&inputs[{a_idx}][i]);")
            self.code_buffer.append(f"      float32x4_t v_act_b = vld1q_f32(&inputs[{b_idx}][i]);")
            self.code_buffer.append(f"      float32x4_t v_g_a = vld1q_f32(&grads[{a_idx}][i]);")
            self.code_buffer.append(f"      float32x4_t v_g_b = vld1q_f32(&grads[{b_idx}][i]);")
            # v_g_a += v_grad_out / v_act_b
            self.code_buffer.append(f"      float32x4_t v_tmp = vdivq_f32(v_grad_out, v_act_b);")
            self.code_buffer.append(f"      v_g_a = vaddq_f32(v_g_a, v_tmp);")
            self.code_buffer.append(f"      vst1q_f32(&grads[{a_idx}][i], v_g_a);")
            # v_g_b -= v_grad_out * v_act_a / (v_act_b * v_act_b)
            self.code_buffer.append(f"      float32x4_t v_b2 = vmulq_f32(v_act_b, v_act_b);")
            self.code_buffer.append(f"      float32x4_t v_tmp2 = vdivq_f32(v_act_a, v_b2);")
            self.code_buffer.append(f"      v_tmp2 = vmulq_f32(v_grad_out, v_tmp2);")
            self.code_buffer.append(f"      v_g_b = vsubq_f32(v_g_b, v_tmp2);")
            self.code_buffer.append(f"      vst1q_f32(&grads[{b_idx}][i], v_g_b);")

            
        elif node.op_type == Operators.RELU:
            input_node = node.inputs[0]
            in_idx = self.execution_order.index(input_node)
            # 1. Load data
            self.code_buffer.append(f"      float32x4_t v_act_in = vld1q_f32(&inputs[{in_idx}][i]);")
            self.code_buffer.append(f"      float32x4_t v_g_in = vld1q_f32(&grads[{in_idx}][i]);")
            self.code_buffer.append(f"      float32x4_t v_zero = vdupq_n_f32(0.0f);")
            
            # 2. Create Mask (x > 0)
            self.code_buffer.append(f"      uint32x4_t mask = vcgtq_f32(v_act_in, v_zero);")
            
            # 3. Bitwise AND (Corrected casting for NEON)
            self.code_buffer.append(f"      uint32x4_t v_g_out_u32 = vreinterpretq_u32_f32(v_grad_out);")
            self.code_buffer.append(f"      float32x4_t v_grad_masked = vreinterpretq_f32_u32(vandq_u32(v_g_out_u32, mask));")
            
            # 4. Accumulate
            self.code_buffer.append(f"      v_g_in = vaddq_f32(v_g_in, v_grad_masked);")
            self.code_buffer.append(f"      vst1q_f32(&grads[{in_idx}][i], v_g_in);")

    def _emit_backward_node_code(self, node: Node) -> None:
        node_idx = self.execution_order.index(node)
        if node.op_type == Operators.ADD:
            for input_node in node.inputs: 
                in_idx = self.execution_order.index(input_node)
                self.code_buffer.append(f"          grads[{in_idx}][i] += s_grad_out;")
        elif node.op_type == Operators.SUB: 
            left_node, right_node = node.inputs
            left_idx = self.execution_order.index(left_node)
            right_idx = self.execution_order.index(right_node)
            self.code_buffer.append(f"          grads[{left_idx}][i] += s_grad_out;")
            self.code_buffer.append(f"          grads[{right_idx}][i] -= s_grad_out;")

        elif node.op_type == Operators.MUL: 
            a_node, b_node = node.inputs
            a_idx, b_idx = self.execution_order.index(a_node), self.execution_order.index(b_node)
            self.code_buffer.append(f"          grads[{a_idx}][i] += s_grad_out * inputs[{b_idx}][i];")
            self.code_buffer.append(f"          grads[{b_idx}][i] += s_grad_out * inputs[{a_idx}][i];")
        
        elif node.op_type == Operators.DIV: 
            a_node, b_node = node.inputs
            a_idx, b_idx = self.execution_order.index(a_node), self.execution_order.index(b_node)
            self.code_buffer.append(f"          grads[{a_idx}][i] += s_grad_out / inputs[{b_idx}][i];")
            self.code_buffer.append(f"          grads[{b_idx}][i] -= s_grad_out * inputs[{a_idx}][i] / (inputs[{b_idx}][i] * inputs[{b_idx}][i]);")
        elif node.op_type == Operators.RELU:
            input_node = node.inputs[0]
            in_idx = self.execution_order.index(input_node)
            self.code_buffer.append(f"          if (inputs[{in_idx}][i] > 0)  grads[{in_idx}][i] += s_grad_out;")

    def compile_matmul_backward(self, node: Node) -> str: 
        input_a, input_b = node.inputs
        M,K = input_a.shape
        _, N = input_b.shape

        code = [
                "// MatMul Backward Kernel generated by VectorJIT",
                "extern \"C\" void compute_backward(float** grads, float** inputs, int n, float* grad_out) {",
                f"    float* grad_a = grads[0]; float* grad_b = grads[1];",
                f"    float* A = inputs[0]; float* B = inputs[1];",
                "",
                "    // 1. grad_A = grad_out @ B.T (Shape: M x K) [cite: 80]",
                "    #pragma omp parallel for collapse(2)",
                f"    for (int i = 0; i < {M}; ++i) {{",
                f"        for (int j = 0; j < {K}; ++j) {{",
                "            float sum = 0.0f;",
                f"            for (int l = 0; l < {N}; ++l) {{",
                f"                sum += grad_out[i * {N} + l] * B[j * {N} + l];",
                "            }",
                f"            grad_a[i * {K} + j] += sum;",
                "        }",
                "    }",
                "",
                "    // 2. grad_B = A.T @ grad_out (Shape: K x N) ",
                "    #pragma omp parallel for collapse(2)",
                f"    for (int i = 0; i < {K}; ++i) {{",
                f"        for (int j = 0; j < {N}; ++j) {{",
                "            float sum = 0.0f;",
                f"            for (int l = 0; l < {M}; ++l) {{",
                f"                sum += A[l * {K} + i] * grad_out[l * {N} + j];",
                "            }",
                f"            grad_b[i * {N} + j] += sum;",
                "        }",
                "    }",
                "}" 
        ]
        return "\n".join(code)
    
    def compile_LayerNorm_Backward(self, node: Node)-> str: 
        eps = node.attributes.get('eps', 1e-5)
        # Assuming shape is (Batch, Features)
        B, F = node.shape 
        
        code = [
            "// Fused LayerNorm Backward Kernel",
            "extern \"C\" void layernorm_backward(float* grad_x, float* grad_gamma, float* grad_beta, "
            "                                    float* grad_out, float* x, float* gamma, int B, int F) {",
            "    #pragma omp parallel for",
            f"    for (int i = 0; i < B; ++i) {{",
            "        float sum_grad = 0.0f;",
            "        float sum_grad_xhat = 0.0f;",
            "        float mean = 0.0f;",
            "        float var = 0.0f;",
            "",
            "        // Pass 1: Calculate Mean and Variance for this row",
            "        for (int j = 0; j < F; ++j) { mean += x[i*F + j]; }",
            "        mean /= F;",
            "        for (int j = 0; j < F; ++j) { ",
            "            float diff = x[i*F + j] - mean;",
            "            var += diff * diff;",
            "        }",
            f"        float inv_std = 1.0f / sqrtf((var / F) + {eps}f);",
            "",
            "        // Pass 2: Calculate gradients for gamma, beta, and intermediate input grad",
            "        for (int j = 0; j < F; ++j) {",
            "            float x_hat = (x[i*F + j] - mean) * inv_std;",
            "            float g_out = grad_out[i*F + j];",
            "            grad_gamma[j] += g_out * x_hat; // Accumulate gamma grad",
            "            grad_beta[j] += g_out;         // Accumulate beta grad",
            "            sum_grad += g_out;",
            "            sum_grad_xhat += g_out * x_hat;",
            "        }",
            "",
            "        // Pass 3: Final input gradient dL/dx using the 5-term formula",
            "        for (int j = 0; j < F; ++j) {",
            "            float x_hat = (x[i*F + j] - mean) * inv_std;",
            "            grad_x[i*F + j] += (gamma[j] * inv_std / F) * ",
            "                               (F * grad_out[i*F + j] - sum_grad - x_hat * sum_grad_xhat);",
            "        }",
            "    }",
            "}"
        ]
        return "\n".join(code)
    
    def compile_LayerNorm_SIMD(self, node: Node) -> str: 
        eps = node.attributes.get("eps", 1e-5)
        # Handle both 2D (B, F) and 3D (B, S, E) shapes
        if len(node.shape) == 2:
            B, F = node.shape
        else:
            # For 3D: flatten B*S into batch dimension
            B = node.shape[0] * node.shape[1]
            F = node.shape[2]
        code = [
            "#include <arm_neon.h>", 
            "#include <cmath>", 
            "#include <omp.h>", 
            "", 
            "extern \"C\" void compute(float** vars, int n){", 
            "   float *output = vars[0];",
            "   float *x = vars[1]; float* gamma = vars[2]; float* beta = vars[3];", 
            "   int vector_limit = " + str(F - (F%4)) + ";", 
            "", 
            "   #pragma omp parallel for", 
            f"  for(int i =0; i < {B}; ++i){{", 
            "       float sum = 0.0f; float sum_sq  = 0.0f;", 
            "       for(int j=0; j< " + str(F) + "; ++j){", 
            f"          float val = x[i * {F} + j];", 
            "           sum += val; sum_sq += val*val;", 
            "       }", 
            f"      float mean = sum/{F};",
            f"      float var = (sum_sq/{F})-(mean*mean);", 
            f"      float inv_std=1.0f/sqrt(var+{eps}f);", 
            "       float32x4_t v_mean = vdupq_n_f32(mean);",
            "       float32x4_t v_inv_std = vdupq_n_f32(inv_std);", 
            "       int j=0;",
            "       for (; j< vector_limit; j+=4){", 
            f"           int idx = i * {F} + j;", 
            "            float32x4_t v_x = vld1q_f32(&x[idx]);",
            "            float32x4_t v_gamma = vld1q_f32(&gamma[j]);", 
            "            float32x4_t v_beta = vld1q_f32(&beta[j]);", 
            "            float32x4_t v_x_hat = vmulq_f32(vsubq_f32(v_x, v_mean), v_inv_std);", 
            "            float32x4_t v_out = vfmaq_f32(v_beta, v_x_hat, v_gamma);", 
            "            vst1q_f32(&output[idx], v_out);", 
            "       }", 
            "       // Scalar Tail for non-multiples of 4", 
            "       for(; j < "+str(F)+"; ++j){", 
            f"           int idx = i * {F} + j; ", 
            "            output[idx] = ((x[idx] - mean) * inv_std) * gamma[j] + beta[j];", 
            "        }",
            "    }",
            "}"      
        ]
        return "\n".join(code)
    
    def compile_LayerNorm_Backend_SIMD(self, node):
        """Fixed LayerNorm backward with thread-safe reduction."""
        eps = node.attributes.get('eps', 1e-5)
        if len(node.shape) == 2: B, F = node.shape
        else: B = node.shape[0] * node.shape[1]; F = node.shape[2]
        vector_limit = F - (F % 4)
        
        code = [
            "#include <arm_neon.h>", "#include <cmath>", "#include <omp.h>",
            "#include <cstring>",  # for memset
            "",
            "extern \"C\" void layernorm_backward(float* grad_x, float* grad_gamma, float* grad_beta,",
            "                                    float* grad_out, float* x, float* gamma, int B, int F) {",
            "",
            "   // Thread-local buffers for gamma/beta gradient reduction",
            "   int num_threads = omp_get_max_threads();",
            f"   float local_gg[num_threads][{F}];",  # VLA â€” fine for F=64
            f"   float local_gb[num_threads][{F}];",
            "   for (int t = 0; t < num_threads; ++t) {",
            f"       memset(local_gg[t], 0, {F} * sizeof(float));",
            f"       memset(local_gb[t], 0, {F} * sizeof(float));",
            "   }",
            "",
            "   #pragma omp parallel",
            "   {",
            "       int tid = omp_get_thread_num();",
            "       #pragma omp for",
            f"       for (int i = 0; i < {B}; ++i) {{",
            "           float row_sum_grad = 0.0f; float row_sum_grad_xhat = 0.0f;",
            "           float row_sum_x = 0.0f; float row_sum_xsq = 0.0f;",
            "           int offset = i * F;",
            f"           for (int j = 0; j < {F}; ++j) {{",
            "               float val = x[offset + j]; row_sum_x += val; row_sum_xsq += val*val;",
            "           }",
            f"           float mean = row_sum_x / {F};",
            f"           float var = (row_sum_xsq / {F}) - (mean * mean);",
            f"           float inv_std = 1.0f / sqrtf(var + {eps}f);",
            "",
            "           // Pass 2: accumulate into thread-local gamma/beta grads",
            "           float32x4_t v_sum_grad = vdupq_n_f32(0.0f);",
            "           float32x4_t v_sum_grad_xhat = vdupq_n_f32(0.0f);",
            "           float32x4_t v_mean = vdupq_n_f32(mean);",
            "           float32x4_t v_inv_std = vdupq_n_f32(inv_std);",
            "           int j = 0;",
            f"           for (; j < {vector_limit}; j += 4) {{",
            "               float32x4_t v_x = vld1q_f32(&x[offset + j]);",
            "               float32x4_t v_g_out = vld1q_f32(&grad_out[offset + j]);",
            "               float32x4_t v_x_hat = vmulq_f32(vsubq_f32(v_x, v_mean), v_inv_std);",
            "               v_sum_grad = vaddq_f32(v_sum_grad, v_g_out);",
            "               v_sum_grad_xhat = vfmaq_f32(v_sum_grad_xhat, v_g_out, v_x_hat);",
            "               float32x4_t v_gg = vld1q_f32(&local_gg[tid][j]);",
            "               float32x4_t v_gb = vld1q_f32(&local_gb[tid][j]);",
            "               vst1q_f32(&local_gg[tid][j], vfmaq_f32(v_gg, v_g_out, v_x_hat));",
            "               vst1q_f32(&local_gb[tid][j], vaddq_f32(v_gb, v_g_out));",
            "           }",
            "           row_sum_grad = vaddvq_f32(v_sum_grad);",
            "           row_sum_grad_xhat = vaddvq_f32(v_sum_grad_xhat);",
            f"           for (; j < {F}; ++j) {{",
            "               float val = x[offset + j]; float g_out = grad_out[offset + j];",
            "               float x_hat = (val - mean) * inv_std;",
            "               row_sum_grad += g_out; row_sum_grad_xhat += g_out * x_hat;",
            "               local_gg[tid][j] += g_out * x_hat;",
            "               local_gb[tid][j] += g_out;",
            "           }",
            "",
            "           // Pass 3: grad_x (per-row, no race)",
            "           float32x4_t v_s_grad = vdupq_n_f32(row_sum_grad);",
            "           float32x4_t v_s_grad_xhat = vdupq_n_f32(row_sum_grad_xhat);",
            f"           float32x4_t v_F_inv = vdupq_n_f32(1.0f / {F}.0f);",
            "           j = 0;",
            f"           for (; j < {vector_limit}; j += 4) {{",
            "               float32x4_t v_x = vld1q_f32(&x[offset + j]);",
            "               float32x4_t v_g_out = vld1q_f32(&grad_out[offset + j]);",
            "               float32x4_t v_gamma = vld1q_f32(&gamma[j]);",
            "               float32x4_t v_x_hat = vmulq_f32(vsubq_f32(v_x, v_mean), v_inv_std);",
            f"               float32x4_t v_term1 = vmulq_f32(vmulq_f32(v_gamma, v_inv_std), v_F_inv);",
            f"               float32x4_t v_term2 = vsubq_f32(vsubq_f32(vmulq_f32(vdupq_n_f32({F}.0f), v_g_out), v_s_grad), vmulq_f32(v_x_hat, v_s_grad_xhat));",
            "               vst1q_f32(&grad_x[offset + j], vmulq_f32(v_term1, v_term2));",
            "           }",
            f"           for (; j < {F}; ++j) {{",
            "               float x_hat = (x[offset + j] - mean) * inv_std;",
            f"               float term = ({F} * grad_out[offset + j] - row_sum_grad - x_hat * row_sum_grad_xhat);",
            f"               grad_x[offset + j] = (gamma[j] * inv_std / {F}) * term;",
            "           }",
            "       }",  # end for i
            "   }",  # end parallel
            "",
            "   // Merge thread-local gamma/beta grads (single-threaded, no race)",
            "   for (int t = 0; t < num_threads; ++t) {",
            f"       for (int j = 0; j < {F}; ++j) {{",
            "           grad_gamma[j] += local_gg[t][j];",
            "           grad_beta[j] += local_gb[t][j];",
            "       }",
            "   }",
            "}"
        ]
        return "\n".join(code)

    def compile_fused_loss_forward(self, node: Node) -> str:
        # Check dimensionality. If 3D (B, S, E), we assume user flattened or we handle 2D.
        # This kernel expects strictly 2D (N, C) inputs where C is classes.
        shape = node.inputs[0].shape
        if len(shape) == 2:
            B, F = shape
        else:
            # Fallback for 3D: treat B*S as batch dimension
            B = shape[0] * shape[1]
            F = shape[2]
            
        code = [
            "#include <arm_neon.h>", "#include <cmath>", "#include <omp.h>",
            "extern \"C\" void compute(float** vars, int n) {",
            "    float* loss_out = vars[0]; float* logits = vars[1]; float* targets = vars[2];",
            "    #pragma omp parallel for",
            f"    for (int i = 0; i < {B}; ++i) {{",
            f"        int offset = i * {F};",
            "        float max_l = logits[offset];",
            f"        for (int j = 1; j < {F}; ++j) if (logits[offset+j] > max_l) max_l = logits[offset+j];",
            "        float sum_e = 0.0f;",
            f"        for (int j = 0; j < {F}; ++j) sum_e += expf(logits[offset+j] - max_l);",
            "        int target_idx = (int)targets[i];",
            "        loss_out[i] = (max_l + logf(sum_e)) - logits[offset + target_idx];",
            "    }",
            "}"
        ]
        return "\n".join(code)
    
    def compile_fused_loss_backward(self, node: Node) -> str:
        shape = node.inputs[0].shape
        if len(shape) == 2: B, F = shape
        else: B = shape[0] * shape[1]; F = shape[2]
        code = [
            "#include <arm_neon.h>", "#include <cmath>", "#include <omp.h>",
            "extern \"C\" void loss_backward(float** grads, float** inputs, int B, float* grad_out) {",
            "    float* grad_logits = grads[0]; float* logits = inputs[0]; float* targets = inputs[1];",
            f"    int F = {F};",
            "   float inv_B = 1.0f / B;",  # Normalize gradients by batch size
            "    #pragma omp parallel for",
            "    for (int i = 0; i < B; ++i) {",
            "        int offset = i * F;",
            "        float max_l = logits[offset];",
            "        for (int j = 1; j < F; ++j) { if(logits[offset+j] > max_l) max_l = logits[offset+j]; }",
            "        float sum_e = 0.0f;",
            "        for (int j = 0; j < F; ++j) { sum_e += expf(logits[offset+j] - max_l); }",
            "        int target_idx = (int)targets[i];",
            "        float g_out_val = grad_out[i] * inv_B;",  # Normalize gradient by batch size
            "        for (int j = 0; j < F; ++j) {",
            "            float prob = expf(logits[offset+j] - max_l) / sum_e;",
            "            grad_logits[offset + j] = (j == target_idx) ? (prob - 1.0f) : prob;",
            "            grad_logits[offset + j] *= g_out_val;",
            "        }",
            "    }",
            "}"
        ]
        return "\n".join(code)
    
    def compile_mha_forward(self, node: Node)-> str: 
        B, S, E = node.shape
        num_heads = node.attributes['num_heads']
        head_dim = E // num_heads
        scale = 1.0 / np.sqrt(head_dim)
        vec_limit = head_dim - (head_dim % 4)
        code = [
            "#include <arm_neon.h>",
            "#include <cmath>",
            "#include <omp.h>",
            "",
            "extern \"C\" void compute(float** vars, int n) {",
            "    float* output = vars[0]; float* Q = vars[1]; float* K = vars[2]; float* V = vars[3];",
            f"    const float scale_val = {scale}f;",
            "    float32x4_t v_scale = vdupq_n_f32(scale_val);",
            "",
            "    #pragma omp parallel for collapse(2)",
            f"    for (int b = 0; b < {B}; ++b) {{",
            f"        for (int h = 0; h < {num_heads}; ++h) {{",
            f"            for (int i = 0; i < {S}; ++i) {{",
            f"                float scores[{S}];",
            "                float max_score = -1e9f;",
            "",
            "                // 1. Vectorized Scaled Dot-Product (Q @ K.T)",
            f"                for (int j = 0; j < {S}; ++j) {{",
            "                    float32x4_t v_acc = vdupq_n_f32(0.0f);",
            "                    int k = 0;",
            f"                    for (; k < {vec_limit}; k += 4) {{",
            f"                        int q_idx = (b * {S} * {E}) + (i * {E}) + (h * {head_dim}) + k;",
            f"                        int k_idx = (b * {S} * {E}) + (j * {E}) + (h * {head_dim}) + k;",
            "                        float32x4_t v_q = vld1q_f32(&Q[q_idx]);",
            "                        float32x4_t v_k = vld1q_f32(&K[k_idx]);",
            "                        v_acc = vfmaq_f32(v_acc, v_q, v_k);",
            "                    }",
            "                    float dot = vaddvq_f32(v_acc);",
            "                    // Scalar cleanup for head_dim",
            f"                    for (; k < {head_dim}; ++k) {{",
            f"                        int q_idx = (b * {S} * {E}) + (i * {E}) + (h * {head_dim}) + k;",
            f"                        int k_idx = (b * {S} * {E}) + (j * {E}) + (h * {head_dim}) + k;",
            "                        dot += Q[q_idx] * K[k_idx];",
            "                    }",
            "                    scores[j] = dot * scale_val;",
            "                    if (scores[j] > max_score) max_score = scores[j];",
            "                }",
            "",
            "                // 2. Numerically Stable Softmax (Simplified for sequence length)",
            "                float sum_exp = 0.0f;",
            f"                for (int j = 0; j < {S}; ++j) {{",
            "                    scores[j] = expf(scores[j] - max_score);",
            "                    sum_exp += scores[j];",
            "                }",
            "",
            "                // 3. Vectorized Weighted Sum (Attention @ V)",
            f"                for (int j = 0; j < {S}; ++j) {{",
            "                    float prob = scores[j] / sum_exp;",
            "                    float32x4_t v_prob = vdupq_n_f32(prob);",
            "                    int k = 0;",
            f"                    for (; k < {vec_limit}; k += 4) {{",
            f"                        int v_idx = (b * {S} * {E}) + (j * {E}) + (h * {head_dim}) + k;",
            f"                        int out_idx = (b * {S} * {E}) + (i * {E}) + (h * {head_dim}) + k;",
            "                        float32x4_t v_v = vld1q_f32(&V[v_idx]);",
            "                        float32x4_t v_out = vld1q_f32(&output[out_idx]);",
            "                        vst1q_f32(&output[out_idx], vfmaq_f32(v_out, v_prob, v_v));",
            "                    }",
            "                    // Scalar cleanup for output",
            f"                    for (; k < {head_dim}; ++k) {{",
            f"                        int v_idx = (b * {S} * {E}) + (j * {E}) + (h * {head_dim}) + k;",
            f"                        int out_idx = (b * {S} * {E}) + (i * {E}) + (h * {head_dim}) + k;",
            "                        output[out_idx] += prob * V[v_idx];",
            "                    }",
            "                }",
            "            }",
            "        }",
            "    }",
            "}"
        ]

        return "\n".join(code)
    
    def compile_mha_backward(self, node: Node) -> str: 
        B, S, E = node.shape
        num_heads = node.attributes['num_heads'] 
        head_dim = E // num_heads
        scale = 1 / np.sqrt(head_dim)
        vec_limit = head_dim - (head_dim % 4)  

        code = [
            "#include <arm_neon.h>", "#include <cmath>", "#include <omp.h>",
            "extern \"C\" void mha_backward(float** grads, float** inputs, int n, float* grad_out) {",
            "    float* gQ = grads[0]; float* gK = grads[1]; float* gV = grads[2];",
            "    float* Q = inputs[0];  float* K = inputs[1];  float* V = inputs[2];",
            f"    const float scale_val = {scale}f;",
            "",
            "    #pragma omp parallel for collapse(2)",
            f"    for (int b = 0; b < {B}; ++b) {{",
            f"        for (int h = 0; h < {num_heads}; ++h) {{",
            f"            for (int i = 0; i < {S}; ++i) {{",
            f"                float probs[{S}]; float dS[{S}];",
            f"                float* q_row = &Q[(b * {S} * {E}) + (i * {E}) + (h * {head_dim})];",
            f"                float* gq_row = &gQ[(b * {S} * {E}) + (i * {E}) + (h * {head_dim})];",
            f"                float* gout_row = &grad_out[(b * {S} * {E}) + (i * {E}) + (h * {head_dim})];",
            "",
            "                // 1. Recompute Probs (Simplified for speed)",
            "                float max_s = -1e9f;",
            f"                for (int j = 0; j < {S}; ++j) {{",
            f"                    float* k_row = &K[(b * {S} * {E}) + (j * {E}) + (h * {head_dim})];",
            "                    float dot = 0.0f; int k=0;",
            f"                    if ({head_dim} >= 4) {{",
            "                        float32x4_t v_acc = vdupq_n_f32(0.0f);",
            f"                        for (; k < {vec_limit}; k += 4) v_acc = vfmaq_f32(v_acc, vld1q_f32(q_row+k), vld1q_f32(k_row+k));",
            "                        dot = vaddvq_f32(v_acc);",
            "                    }",
            f"                    for (; k < {head_dim}; ++k) dot += q_row[k] * k_row[k];",
            "                    probs[j] = dot * scale_val;",
            "                    if (probs[j] > max_s) max_s = probs[j];",
            "                }",
            "                float sum_e = 0.0f;",
            f"                for (int j = 0; j < {S}; ++j) {{", 
            "                   probs[j] = expf(probs[j] - max_s); sum_e += probs[j];",
            "                }",
            f"                for (int j = 0; j < {S}; ++j) probs[j] /= sum_e;",
            "",
            "                // 2. Vectorized gV and dS",
            f"                float row_sum_dP = 0.0f;",
            f"                for (int j = 0; j < {S}; ++j) {{",
            f"                    float* v_row = &V[(b * {S} * {E}) + (j * {E}) + (h * {head_dim})];",
            f"                    float* gv_row = &gV[(b * {S} * {E}) + (j * {E}) + (h * {head_dim})];",
            "                    float32x4_t v_p = vdupq_n_f32(probs[j]);",
            "                    float32x4_t v_dp_acc = vdupq_n_f32(0.0f);",
            "                    int k = 0;",
            f"                    for (; k < {vec_limit}; k += 4) {{",
            "                        float32x4_t v_go = vld1q_f32(gout_row + k);",
            "                        vst1q_f32(gv_row+k, vfmaq_f32(vld1q_f32(gv_row+k), v_p, v_go));",
            "                        v_dp_acc = vfmaq_f32(v_dp_acc, v_go, vld1q_f32(v_row+k));",
            "                    }",
            "                    float dp_j = vaddvq_f32(v_dp_acc);",
            f"                    for (; k < {head_dim}; ++k) {{",  
            "                        gv_row[k] += probs[j] * gout_row[k]; dp_j += gout_row[k] * v_row[k];", 
            "                    }",
            "                    dS[j] = dp_j; row_sum_dP += probs[j] * dp_j;",
            "                }",
            "",
            "                // 3. Vectorized gQ and gK",
            f"                for (int j = 0; j < {S}; ++j) {{",
            f"                    float* k_row = &K[(b * {S} * {E}) + (j * {E}) + (h * {head_dim})];",
            f"                    float* gk_row = &gK[(b * {S} * {E}) + (j * {E}) + (h * {head_dim})];",
            "                    float32x4_t v_grad_s = vdupq_n_f32(scale_val * probs[j] * (dS[j] - row_sum_dP));",
            "                    int k = 0;",
            f"                    for (; k < {vec_limit}; k += 4) {{",
            "                        vst1q_f32(gq_row+k, vfmaq_f32(vld1q_f32(gq_row+k), v_grad_s, vld1q_f32(k_row+k)));",
            "                        vst1q_f32(gk_row+k, vfmaq_f32(vld1q_f32(gk_row+k), v_grad_s, vld1q_f32(q_row+k)));",
            "                    }",
            f"                    for (; k < {head_dim}; ++k) {{",
            "                        float gs = vgetq_lane_f32(v_grad_s, 0);",
            "                        gq_row[k] += gs * k_row[k]; gk_row[k] += gs * q_row[k];",
            "                    }",
            "                }",
            "            }",
            "        }",
            "    }",
            "}"
        ]
        return "\n".join(code)
    
    def compile_GELU_forward_SIMD(self, node: Node) -> str: 
        total_elements = int(np.prod(node.shape))
        vec_limit = total_elements - (total_elements % 4)
        code = [
            "#include <arm_neon.h>", 
            "#include <cmath>", 
            "#include <omp.h>", 
            "", 
            "extern \"C\" void compute(float** vars, int n) {",
            "   float* output = vars[0];", 
            "   float* input = vars[1]; ", 
            "   const float c1 = 0.797885; // sqrt(2/pi)", 
            "   const float a = 0.44715; // a-> optained via numeric optimization", 
            "   float32x4_t v_c1 = vdupq_n_f32(c1);", 
            "   float32x4_t v_a = vdupq_n_f32(a);", 
            "   float32x4_t v_one = vdupq_n_f32(1.0f);", 
            "   float32x4_t v_half = vdupq_n_f32(0.5f);", 
            "   float32x4_t v_fif = vdupq_n_f32(15.0f);", 
            "   float32x4_t v_six = vdupq_n_f32(6.0f);", 
            "   #pragma omp parallel for",
            f"   for(int i = 0; i < {vec_limit}; i+=4){{",
            "       float32x4_t x = vld1q_f32(input+i);",
            "       float32x4_t x3 = vmulq_f32(vmulq_f32(x, x), x);",
            "       float32x4_t inner = vmulq_f32(v_c1, vfmaq_f32(x, v_a, x3));",
            "       float32x4_t y2 = vmulq_f32(inner, inner);", 
            "       float32x4_t y3 = vmulq_f32(inner, y2);", 
            "       float32x4_t upper = vaddq_f32(vmulq_f32(v_fif, inner), y3);", 
            "       float32x4_t lower = vaddq_f32(v_fif, vmulq_f32(v_six, y2));", 
            "       float32x4_t tanh_approx = vdivq_f32(upper, lower);", 
            "       float32x4_t gelu_approx = vmulq_f32(vmulq_f32(v_half, x), vaddq_f32(v_one, tanh_approx));",
            "       vst1q_f32(output+i, gelu_approx);",
            "   }", 
            "    // Scalar cleanup",
            f"    for (int i = {vec_limit}; i < {total_elements}; ++i) {{",
            "        float x = input[i];",
            "        output[i] = 0.5f * x * (1.0f + tanhf(c1 * (x + a * x * x * x)));",
            "    }",
            "}",  

        ]
        return "\n".join(code)
    
    def compile_GELU_Backward_SIMD(self, node: Node) -> str:
        total_elements = int(np.prod(node.shape))
        vec_limit = total_elements - (total_elements % 4)

        code = [
            "#include <arm_neon.h>",
            "#include <cmath>",
            "#include <omp.h>",
            "",
            "extern \"C\" void gelu_backward(float** grads, float** inputs, int n, float* grad_out) {",
            "    float* grad_in = grads[0];  // Gradient to propagate back",
            "    float* input = inputs[0];   // Original forward input",
            "    ",
            "    const float c1 = 0.797885f; // sqrt(2/pi)",
            "    const float a = 0.44715f;",
            "    float32x4_t v_c1 = vdupq_n_f32(c1);",
            "    float32x4_t v_a = vdupq_n_f32(a);",
            "    float32x4_t v_one = vdupq_n_f32(1.0f);",
            "    float32x4_t v_half = vdupq_n_f32(0.5f);",
            "    float32x4_t v_fif = vdupq_n_f32(15.0f);", 
            "    float32x4_t v_six = vdupq_n_f32(6.0f);",
            "    ",
            "    #pragma omp parallel for",
            f"    for(int i = 0; i < {vec_limit}; i += 4) {{",
            "        float32x4_t x = vld1q_f32(input + i);",
            "        float32x4_t gout = vld1q_f32(grad_out + i);",
            "        ",
            "        // 1. Recompute inner term",
            "        float32x4_t x2 = vmulq_f32(x, x);",
            "        float32x4_t inner = vmulq_f32(v_c1, vfmaq_f32(x, v_a, vmulq_f32(x2, x)));",
            "        ",
            "        // 2. PadÃ© Tanh Approximation",
            "        float32x4_t y2 = vmulq_f32(inner, inner);",
            "        float32x4_t y3 = vmulq_f32(inner, y2);",
            "        float32x4_t tanh_res = vdivq_f32(vfmaq_f32(y3, v_fif, inner), vfmaq_f32(v_fif, v_six, y2));",
            "        ",
            "        // 3. GELU Derivative Approximation: 0.5 * (1 + tanh(y) + x * sech^2(y) * dy/dx)",
            "        // We use a simplified stable version: 0.5 * (1 + tanh + (x * 0.797 * (1 + 3 * 0.447 * x^2)) * (1 - tanh^2))",
            "        float32x4_t sech2 = vsubq_f32(v_one, vmulq_f32(tanh_res, tanh_res));",
            "        float32x4_t dy_dx = vmulq_f32(v_c1, vfmaq_f32(v_one, vdupq_n_f32(3.0f * 0.44715f), x2));",
            "        float32x4_t local_grad = vmulq_f32(v_half, vaddq_f32(vaddq_f32(v_one, tanh_res), vmulq_f32(vmulq_f32(x, dy_dx), sech2)));",
            "        ",
            "        // 4. Chain rule: grad_in += grad_out * local_grad",
            "        vst1q_f32(grad_in + i, vfmaq_f32(vld1q_f32(grad_in + i), gout, local_grad));",
            "    }",
            "    ",
            "    // Scalar cleanup",
            f"    for(int i = {vec_limit}; i < {total_elements}; ++i) {{",
            "        float x = input[i];",
            "        float inner = c1 * (x + a * x * x * x);",
            "        float t = tanhf(inner);",
            "        float dg = 0.5f * (1.0f + t + x * (1.0f - t * t) * (c1 * (1.0f + 3.0f * a * x * x)));",
            "        grad_in[i] += grad_out[i] * dg;",
            "    }",
            "}"
        ]
        return "\n".join(code)

    def compile_FFN_forward_SIMD(self, node: Node) -> str:
        B, S, E = node.inputs[0].shape 
        H = node.attributes['hidden_dim']
        vec_limit_H = H - (H % 4)

        code = [
            "#include <arm_neon.h>", "#include <cmath>", "#include <omp.h>",
            "extern \"C\" void compute(float** vars, int n) {", 
            "   float* output = vars[0]; float* X = vars[1];",
            "   float* W1 = vars[2];     float* b1 = vars[3];",
            "   float* W2_T = vars[4];   float* b2 = vars[5];", # Named W2_T to match your loop
            "   const float c1 = 0.797885f; const float a_const = 0.44715f;",
            "   float32x4_t v_c1 = vdupq_n_f32(c1);", 
            "   float32x4_t v_a = vdupq_n_f32(a_const);", 
            "   float32x4_t v_one = vdupq_n_f32(1.0f);", 
            "   float32x4_t v_half = vdupq_n_f32(0.5f);", 
            "   float32x4_t v_fif = vdupq_n_f32(15.0f);", 
            "   float32x4_t v_six = vdupq_n_f32(6.0f);", 
            "",
            "   #pragma omp parallel for collapse(2)", 
            f"   for(int b = 0; b < {B}; ++b) {{",
            f"       for(int s = 0; s < {S}; ++s) {{",
            f"           float* x_ptr = &X[(b * {S} * {E}) + (s * {E})];",
            f"           float* out_ptr = &output[(b * {S} * {E}) + (s * {E})];",
            "",
            "           // 1. Initialize output row with b2",
            f"           for(int e = 0; e < {E}; ++e) out_ptr[e] = b2[e];",
            "",
            "           // 2. Vectorized Expansion + GELU + Contraction",
            f"           for(int h = 0; h < {vec_limit_H}; h += 4) {{", 
            "               float32x4_t v_acc = vld1q_f32(&b1[h]);", 
            f"               for (int e = 0; e < {E}; ++e) {{",
            "                   float32x4_t v_x = vdupq_n_f32(x_ptr[e]);",
            f"                   float32x4_t v_w1 = vld1q_f32(&W1[e * {H} + h]);", 
            "                   v_acc = vfmaq_f32(v_acc, v_x, v_w1);", 
            "               }", 
            "", 
            "               float32x4_t x3 = vmulq_f32(vmulq_f32(v_acc, v_acc), v_acc);",
            "               float32x4_t inner = vmulq_f32(v_c1, vfmaq_f32(v_acc, v_a, x3));",
            "               float32x4_t y2 = vmulq_f32(inner, inner);", 
            "               float32x4_t y3 = vmulq_f32(inner, y2);", 
            "               float32x4_t tanh_approx = vdivq_f32(vfmaq_f32(y3, v_fif, inner), vfmaq_f32(v_fif, v_six, y2));", 
            "               float32x4_t v_gelu = vmulq_f32(vmulq_f32(v_half, v_acc), vaddq_f32(v_one, tanh_approx));",
            "",
            f"               for (int e = 0; e < {E}; ++e) {{",
            f"                   float32x4_t v_w2 = vld1q_f32(&W2_T[e * {H} + h]);", 
            "                   out_ptr[e] += vaddvq_f32(vmulq_f32(v_gelu, v_w2));", 
            "               }", 
            "           }", 
            "",
            "           // 3. Scalar tail for hidden_dim (Now inside the token loop!)",
            f"           for(int h = {vec_limit_H}; h < {H}; ++h) {{",
            "               float val_h = b1[h];", 
            f"              for(int e = 0; e < {E}; ++e) val_h += x_ptr[e] * W1[e * {H} + h];",
            "               float t_inner = c1 * (val_h + a_const * val_h * val_h * val_h);",
            "               float gelu_h = 0.5f * val_h * (1.0f + tanhf(t_inner));",
            f"              for(int e = 0; e < {E}; ++e) out_ptr[e] += gelu_h * W2_T[e * {H} + h];",
            "           }",
            "       }",
            "   }",
            "}"
        ]
        return "\n".join(code)
    
    def compile_FFN_backward_SIMD(self, node: Node) -> str: 
        B, S, E = node.inputs[0].shape
        H = node.attributes['hidden_dim']
        vec_limit_H = H - (H % 4)
        
        # FIX: Use VLA (Stack) instead of std::vector (Heap) to prevent OpenMP heap corruption
        code = [
            "#include <arm_neon.h>", "#include <omp.h>", "#include <cmath>", 
            "extern \"C\" void compute_backward(float** grads, float** inputs, int n, float* grad_out) {", 
            "   float* gX = grads[0];   float* gW1 = grads[1];  float* gb1 = grads[2];", 
            "   float* gW2 = grads[3];  float* gb2 = grads[4];", 
            "   float* X = inputs[0];    float* W1 = inputs[1];   float* b1 = inputs[2];", 
            "   float* W2_T = inputs[3]; float* b2 = inputs[4];", 
            "   const float c1 = 0.797885f; const float a_const = 0.44715f;",
            "   float32x4_t v_c1 = vdupq_n_f32(c1); float32x4_t v_a = vdupq_n_f32(a_const);", 
            "   float32x4_t v_one = vdupq_n_f32(1.0f); float32x4_t v_half = vdupq_n_f32(0.5f);", 
            "   float32x4_t v_fif = vdupq_n_f32(15.0f); float32x4_t v_six = vdupq_n_f32(6.0f);", 
            "   #pragma omp parallel for collapse(2)", 
            f"   for(int b=0; b < {B}; b++) {{", 
            f"       for(int s = 0; s < {S}; ++s) {{", 
            f"           float* x_ptr = &X[(b * {S} * {E}) + (s * {E})];",
            f"           float* gout_ptr = &grad_out[(b * {S} * {E}) + (s * {E})];",
            f"           float hidden[{H}]; float gelu_act[{H}];",
            f"           float tanh_save[{H}]; float dL_dh_save[{H}];",
            "",
            "           // 1. Recompute Hidden & GELU",
            f"           for(int h=0; h < {vec_limit_H}; h+=4) {{", 
            "               float32x4_t v_h = vld1q_f32(&b1[h]);", 
            f"               for(int e = 0; e < {E}; ++e) {{", 
            f"                   v_h = vfmaq_f32(v_h, vdupq_n_f32(x_ptr[e]), vld1q_f32(&W1[e * {H} + h]));", 
            "               }", 
            "               vst1q_f32(&hidden[h], v_h);",
            "               float32x4_t x3 = vmulq_f32(v_h, vmulq_f32(v_h, v_h));", 
            "               float32x4_t inner = vmulq_f32(v_c1, vfmaq_f32(v_h, v_a, x3));", 
            "               float32x4_t y2 = vmulq_f32(inner, inner);", 
            "               float32x4_t t = vdivq_f32(vfmaq_f32(vmulq_f32(inner, y2), v_fif, inner), vfmaq_f32(v_fif, v_six, y2));", 
            "               vst1q_f32(&tanh_save[h], t);",
            "               vst1q_f32(&gelu_act[h], vmulq_f32(vmulq_f32(v_half, v_h), vaddq_f32(v_one, t)));", 
            "           }", 
            f"           for(int h={vec_limit_H}; h < {H}; h++) {{",
            "               float val = b1[h];",
            f"               for(int e=0; e < {E}; e++) val += x_ptr[e] * W1[e * {H} + h];",
            "               hidden[h] = val; float t = tanhf(c1 * (val + a_const * val * val * val));",
            "               tanh_save[h] = t; gelu_act[h] = 0.5f * val * (1.0f + t);",
            "           }",
            "",
            "           // 2. Gradients for W2 and b2",
            f"           for (int e=0; e < {E}; e++) {{", 
            "               float go = gout_ptr[e];",
            "               #pragma omp atomic", 
            "               gb2[e] += go;",
            "               float32x4_t v_go = vdupq_n_f32(go);", 
            f"               for(int h = 0; h < {vec_limit_H}; h+=4) {{",
            "                   float32x4_t v_act = vld1q_f32(&gelu_act[h]);", 
            "                   float32x4_t v_gw2 = vmulq_f32(v_act, v_go);", 
            "                   #pragma omp atomic",
            f"                   gW2[e*{H} + h+0] += vgetq_lane_f32(v_gw2, 0);",
            "                   #pragma omp atomic",
            f"                   gW2[e*{H} + h+1] += vgetq_lane_f32(v_gw2, 1);",
            "                   #pragma omp atomic",
            f"                   gW2[e*{H} + h+2] += vgetq_lane_f32(v_gw2, 2);",
            "                   #pragma omp atomic",
            f"                   gW2[e*{H} + h+3] += vgetq_lane_f32(v_gw2, 3);", 
            "               }", 
            f"               for(int h={vec_limit_H}; h < {H}; ++h) {{",
            "                   #pragma omp atomic",
            f"                  gW2[h*{E} + e] += gelu_act[h] * go;",
            "               }",
            "           }",
            "           // 3. GELU Backprop & W1/b1 Gradients",
            f"           for(int h = 0; h < {vec_limit_H}; h+=4) {{", 
            "               float32x4_t v_dL_dgelu = vdupq_n_f32(0.0f);", 
            f"               for (int e = 0; e < {E}; ++e) {{", 
            f"                   v_dL_dgelu = vfmaq_f32(v_dL_dgelu, vdupq_n_f32(gout_ptr[e]), vld1q_f32(&W2_T[e * {H} + h]));", 
            "               }", 
            "               float32x4_t v_h = vld1q_f32(&hidden[h]); float32x4_t v_t = vld1q_f32(&tanh_save[h]);",
            "               float32x4_t v_dy = vmulq_f32(v_c1, vfmaq_f32(v_one, vdupq_n_f32(3.0f * 0.44715f), vmulq_f32(v_h, v_h)));",
            "               float32x4_t v_dg = vmulq_f32(v_half, vaddq_f32(vaddq_f32(v_one, v_t), vmulq_f32(vmulq_f32(v_h, vsubq_f32(v_one, vmulq_f32(v_t, v_t))), v_dy)));",
            "               float32x4_t v_dL_dh = vmulq_f32(v_dL_dgelu, v_dg);",
            "               #pragma omp atomic",
            "               gb1[h+0] += vgetq_lane_f32(v_dL_dh, 0);",
            "               #pragma omp atomic",
            "               gb1[h+1] += vgetq_lane_f32(v_dL_dh, 1);",
            "               #pragma omp atomic",
            "               gb1[h+2] += vgetq_lane_f32(v_dL_dh, 2);",
            "               #pragma omp atomic",
            "               gb1[h+3] += vgetq_lane_f32(v_dL_dh, 3);",
            "               vst1q_f32(&dL_dh_save[h], v_dL_dh);",
            f"               for(int e=0; e < {E}; ++e) {{",
            "                   float32x4_t v_gw1 = vmulq_f32(vdupq_n_f32(x_ptr[e]), v_dL_dh);",
            "                   #pragma omp atomic",
            f"                   gW1[e * {H} + h + 0] += vgetq_lane_f32(v_gw1, 0);",
            "                   #pragma omp atomic",
            f"                   gW1[e * {H} + h + 1] += vgetq_lane_f32(v_gw1, 1);",
            "                   #pragma omp atomic",
            f"                   gW1[e * {H} + h + 2] += vgetq_lane_f32(v_gw1, 2);",
            "                   #pragma omp atomic",
            f"                   gW1[e * {H} + h + 3] += vgetq_lane_f32(v_gw1, 3);",
            "               }",
            "           }",
            f"           for(int h={vec_limit_H}; h < {H}; ++h) {{",
            "               float dL_dg = 0.0f;", 
            f"              for(int e=0; e<{E}; ++e) dL_dg += gout_ptr[e] * W2_T[e * {H} + h];",
            "               float t = tanh_save[h]; float x = hidden[h];",
            "               float dg = 0.5f * (1.0f + t + x * (1.0f - t * t) * (c1 * (1.0f + 3.0f * a_const * x * x)));",
            "               float dL_dh = dL_dg * dg;", 
            "               #pragma omp atomic", 
            "               gb1[h] += dL_dh;",
            "               dL_dh_save[h] = dL_dh;",
            f"               for(int e=0; e < {E}; ++e) {{",
            "                   #pragma omp atomic",
            f"                  gW1[e * {H} + h] += x_ptr[e] * dL_dh;", 
            "               }",
            "           }",
            "           // 4. Gradient for Input X",
            f"           float* gx_ptr = &gX[(b * {S} * {E}) + (s * {E})];",
            f"           for(int e = 0; e < {E}; ++e) {{",
            "               float32x4_t v_acc = vdupq_n_f32(0.0f);",
            "               int h = 0;",
            f"               for(; h < {vec_limit_H}; h += 4) {{",
            "                   float32x4_t v_dh = vld1q_f32(&dL_dh_save[h]);",
            f"                   float32x4_t v_w1 = vld1q_f32(&W1[e * {H} + h]);",
            "                   v_acc = vfmaq_f32(v_acc, v_dh, v_w1);",
            "               }",
            "               float acc = vaddvq_f32(v_acc);",
            f"              for(; h < {H}; ++h) acc += dL_dh_save[h] * W1[e * {H} + h];",
            "               gx_ptr[e] += acc;", 
            "           }",
            "       }",  
            "   }", 
            "}"
        ]
        return "\n".join(code)
    
    def jit_compile(self, root_node: Node) -> ctypes.CDLL:
        # 1. Generate Source Code
        key = self._get_cache_key(root_node, is_backward=False)
        if key in Compiler._binary_cache:
            if root_node.op_type not in (Operators.MATMUL, Operators.LAYERNORM, Operators.SOFTMAX_CROSS_ENTROPY):
                # Rebuild execution_order and node_idx_map for this graph
                self.visited_nodes.clear()
                self.execution_order.clear()
                self._topological_sort(root_node)
                self.node_idx_map = {node.id: i for i, node in enumerate(self.execution_order)}
            return Compiler._binary_cache[key]

        if root_node.op_type == Operators.MATMUL:
            cpp_code = self.compile_matMul_SIMD(root_node)
        elif root_node.op_type == Operators.LAYERNORM:
            cpp_code = self.compile_LayerNorm_SIMD(root_node)
        elif root_node.op_type == Operators.SOFTMAX_CROSS_ENTROPY:
            cpp_code = self.compile_fused_loss_forward(root_node)
        elif root_node.op_type == Operators.MHA:
            cpp_code = self.compile_mha_forward(root_node)
        elif root_node.op_type == Operators.GELU:
            cpp_code = self.compile_GELU_forward_SIMD(root_node)
        elif root_node.op_type == Operators.FFN:
            cpp_code = self.compile_FFN_forward_SIMD(root_node)
        else:
            cpp_code = self.compile(root_node)
        
        # 2. Write to Temp File
        with tempfile.NamedTemporaryFile(suffix=".cpp", delete=False) as f:
            src_path = f.name
            f.write(cpp_code.encode('utf-8'))

        lib_path = src_path.replace(".cpp", ".so")
            
       # DEBUG: OMP disabled â€” single-threaded, unoptimized for crash isolation
        compile_cmd = [
            "c++", "-O0", "-g", "-shared", "-fPIC", "-march=native",
            "-w",  # suppress unknown-pragma warnings
        ]
        
        # # macOS Detection for OpenMP
        import platform
        if platform.system() == "Darwin":
            # Apple Clang needs special flags for OpenMP
            compile_cmd += ["-Xpreprocessor", "-fopenmp"]
            compile_cmd += ["-lomp"]
            
            # Add Homebrew paths for libomp (Critical for Apple Silicon)
            compile_cmd += ["-I/opt/homebrew/opt/libomp/include"]
            compile_cmd += ["-L/opt/homebrew/opt/libomp/lib"]
        else:
            # Linux/Windows (GCC)
            compile_cmd += ["-fopenmp"]
            
        compile_cmd += [src_path, "-o", lib_path]
        
        # 4. Run Compiler & Capture Output
        result = subprocess.run(compile_cmd, capture_output=True, text=True)
        if result.returncode != 0: 
            print("Compilation failed:")
            print(result.stderr)
            raise RuntimeError("C++ compilation failed")


        # 6. Load Library
        try:
            lib = ctypes.CDLL(lib_path)
            _loaded_libraries.append(lib)
        except OSError as e:
            print(f"Failed to load library at {lib_path}")
            raise e
        
        lib.compute.argtypes = [
            ctypes.POINTER(ctypes.POINTER(ctypes.c_float)), 
            ctypes.c_int
        ]

        Compiler._binary_cache[key] = lib
        os.remove(src_path)
        return lib
    
    def jit_compile_backward(self, root_node: Node) -> tuple:

        key = self._get_cache_key(root_node, is_backward=True)
        if key in Compiler._binary_cache:
            # For generic backward, rebuild execution_order
            if root_node.op_type not in (Operators.MATMUL, Operators.LAYERNORM, Operators.SOFTMAX_CROSS_ENTROPY):
                self.visited_nodes.clear()
                self.execution_order.clear()
                self._topological_sort(root_node)
            lib, func_name = Compiler._binary_cache[key]
            return lib, func_name

        # 2. Dispatch to the correct Generator
        if root_node.op_type == Operators.MATMUL:
            cpp_code = self.compile_matmul_backward(root_node)
            func_name = "compute_backward"
        elif root_node.op_type == Operators.LAYERNORM:
            cpp_code = self.compile_LayerNorm_Backend_SIMD(root_node)
            func_name = "layernorm_backward"
        elif root_node.op_type == Operators.SOFTMAX_CROSS_ENTROPY:
            cpp_code = self.compile_fused_loss_backward(root_node)
            func_name = "loss_backward"
        elif root_node.op_type == Operators.MHA:
            cpp_code = self.compile_mha_backward(root_node)
            func_name = "mha_backward"
        elif root_node.op_type == Operators.GELU: 
            cpp_code = self.compile_GELU_Backward_SIMD(root_node)
            func_name = "gelu_backward"
        elif root_node.op_type == Operators.FFN:
            cpp_code = self.compile_FFN_backward_SIMD(root_node)
            func_name = "compute_backward"
        else:
            cpp_code = self.compile_backward(root_node)
            func_name = "compute_backward"

        # 2. Write to Temp File
        with tempfile.NamedTemporaryFile(suffix=".cpp", delete=False) as f:
            src_path = f.name
            f.write(cpp_code.encode('utf-8'))

        lib_path = src_path.replace(".cpp", ".so")
        
        # 3. Build Compilation Command
               # DEBUG: OMP disabled â€” single-threaded, unoptimized for crash isolation
        compile_cmd = [
            "c++", "-O0", "-g", "-shared", "-fPIC", "-march=native",
            "-w",  # suppress unknown-pragma warnings
        ]
        #compile_cmd += [src_path, "-o", lib_path]
        
        # # macOS Detection for OpenMP
        import platform
        if platform.system() == "Darwin":
            # Apple Clang needs special flags for OpenMP
            compile_cmd += ["-Xpreprocessor", "-fopenmp"]
            compile_cmd += ["-lomp"]
            
            # Add Homebrew paths for libomp (Critical for Apple Silicon)
            compile_cmd += ["-I/opt/homebrew/opt/libomp/include"]
            compile_cmd += ["-L/opt/homebrew/opt/libomp/lib"]
        else:
            # Linux/Windows (GCC)
            compile_cmd += ["-fopenmp"]
            
        compile_cmd += [src_path, "-o", lib_path]
        
        # 4. Run Compiler & Capture Output
        result = subprocess.run(compile_cmd, capture_output=True, text=True)
        if result.returncode != 0: 
            print("Compilation failed:")
            print(result.stderr)
            raise RuntimeError("C++ compilation failed")
        
        try:
            lib = ctypes.CDLL(lib_path)
            _loaded_libraries.append(lib)
        except OSError as e:
            print(f"Failed to load library at {lib_path}")
            raise e
        

        func = getattr(lib, func_name)
        
        if root_node.op_type == Operators.LAYERNORM:
             func.argtypes = [
                ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_float), # grad_x, grad_gamma
                ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_float), # grad_beta, grad_out
                ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_float), # x, gamma
                ctypes.c_int, ctypes.c_int # B, F
            ]
        else:
            # Unified signature for Loss, MatMul, and Element-wise chains
            func.argtypes = [
                ctypes.POINTER(ctypes.POINTER(ctypes.c_float)), # grads array
                ctypes.POINTER(ctypes.POINTER(ctypes.c_float)), # inputs array
                ctypes.c_int,                                   # Batch/Size
                ctypes.POINTER(ctypes.c_float)                  # grad_out
            ]

        # 5. Save to Cache
        Compiler._binary_cache[key] = (lib, func_name)
        os.remove(src_path)
        return lib, func_name